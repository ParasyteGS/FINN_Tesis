{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1692278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --------------------------\n",
    "# Bloques base\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Conv 2D + SiLU\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, k=1, s=1, p=None):\n",
    "        super().__init__()\n",
    "        if p is None:\n",
    "            p = k // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Mini-Bottleneck: 1x1 + 3x3 + Residual\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, shortcut=True, expansion=0.5):\n",
    "        super().__init__()\n",
    "        hidden_ch = int(out_ch * expansion)\n",
    "        self.cv1 = Conv(in_ch, hidden_ch, k=1, s=1)\n",
    "        self.cv2 = Conv(hidden_ch, out_ch, k=3, s=1)\n",
    "        self.use_shortcut = shortcut and in_ch == out_ch\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.cv1(x)\n",
    "        y = self.cv2(y)\n",
    "        if self.use_shortcut:\n",
    "            y = y + x\n",
    "        return y\n",
    "\n",
    "\n",
    "class C2f(nn.Module):\n",
    "    \"\"\"C2f block: Conv 1x1 -> n bottlenecks -> concatenation -> Conv 1x1\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, n=2, expansion=0.5):\n",
    "        super().__init__()\n",
    "        hidden_ch = int(out_ch * expansion)\n",
    "        self.cv1 = Conv(in_ch, hidden_ch, k=1, s=1)\n",
    "        self.bottlenecks = nn.ModuleList(\n",
    "            [\n",
    "                Bottleneck(hidden_ch, hidden_ch, shortcut=True, expansion=1.0)\n",
    "                for _ in range(n)\n",
    "            ]\n",
    "        )\n",
    "        self.cv2 = Conv(hidden_ch * (n + 1), out_ch, k=1, s=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.cv1(x)\n",
    "        outputs = [x1]\n",
    "        for b in self.bottlenecks:\n",
    "            x1 = b(x1)\n",
    "            outputs.append(x1)\n",
    "        y = torch.cat(outputs, dim=1)\n",
    "        return self.cv2(y)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Modelo YOLOv8n-cls completo\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "class YOLOv8nCls(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        # --------------------------\n",
    "        # Backbone\n",
    "        # --------------------------\n",
    "        self.stem = Conv(3, 16, k=3, s=2)  # 224x224x3 -> 112x112x16\n",
    "        self.conv1 = Conv(16, 32, k=3, s=2)  # 112x112x16 -> 56x56x32\n",
    "        self.c2f1 = C2f(32, 32, n=1)  # 56x56x32 -> 56x56x32\n",
    "        self.conv2 = Conv(32, 64, k=3, s=2)  # 56x56x32 -> 28x28x64\n",
    "        self.c2f2 = C2f(64, 64, n=2)  # 28x28x64 -> 28x28x64\n",
    "        self.conv3 = Conv(64, 128, k=3, s=2)  # 28x28x64 -> 14x14x128\n",
    "        self.c2f3 = C2f(128, 128, n=3)  # 14x14x128 -> 14x14x128\n",
    "        self.conv4 = Conv(128, 256, k=3, s=2)  # 14x14x128 -> 7x7x256\n",
    "        self.c2f4 = C2f(256, 256, n=1)  # 7x7x256 -> 7x7x256\n",
    "\n",
    "        # --------------------------\n",
    "        # Head (clasificación)\n",
    "        # --------------------------\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)  # 7x7x256 -> 1x1x256\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.c2f1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.c2f2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.c2f3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.c2f4(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Test rápido\n",
    "# --------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = YOLOv8nCls(num_classes=10)\n",
    "    x = torch.randn(1, 3, 224, 224)\n",
    "    y = model(x)\n",
    "    print(y.shape)  # torch.Size([1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8dcdc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformaciones: ajustar tamaño y normalizar\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "val_dataset = ImageFolder(root=\"dataset_split/val\", transform=val_transform)\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root=\"dataset_split/train\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7896e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 4.9588, Accuracy: 63.88%\n",
      "Epoch 2/50, Loss: 2.2107, Accuracy: 71.62%\n",
      "Epoch 3/50, Loss: 0.8896, Accuracy: 77.25%\n",
      "Epoch 4/50, Loss: 0.6387, Accuracy: 77.25%\n",
      "Epoch 5/50, Loss: 0.4997, Accuracy: 79.12%\n",
      "Epoch 6/50, Loss: 0.4941, Accuracy: 77.25%\n",
      "Epoch 7/50, Loss: 0.4776, Accuracy: 79.12%\n",
      "Epoch 8/50, Loss: 0.4337, Accuracy: 82.62%\n",
      "Epoch 9/50, Loss: 0.4650, Accuracy: 80.12%\n",
      "Epoch 10/50, Loss: 0.4438, Accuracy: 79.38%\n",
      "Epoch 11/50, Loss: 0.4200, Accuracy: 81.62%\n",
      "Epoch 12/50, Loss: 0.4238, Accuracy: 80.00%\n",
      "Epoch 13/50, Loss: 0.4142, Accuracy: 81.75%\n",
      "Epoch 14/50, Loss: 0.4110, Accuracy: 82.00%\n",
      "Epoch 15/50, Loss: 0.4062, Accuracy: 83.00%\n",
      "Epoch 16/50, Loss: 0.4062, Accuracy: 80.62%\n",
      "Epoch 17/50, Loss: 0.4019, Accuracy: 82.00%\n",
      "Epoch 18/50, Loss: 0.3719, Accuracy: 85.12%\n",
      "Epoch 19/50, Loss: 0.3624, Accuracy: 83.88%\n",
      "Epoch 20/50, Loss: 0.4071, Accuracy: 82.62%\n",
      "Epoch 21/50, Loss: 0.3982, Accuracy: 82.00%\n",
      "Epoch 22/50, Loss: 0.3773, Accuracy: 83.38%\n",
      "Epoch 23/50, Loss: 0.3651, Accuracy: 84.50%\n",
      "Epoch 24/50, Loss: 0.3347, Accuracy: 85.62%\n",
      "Epoch 25/50, Loss: 0.3351, Accuracy: 86.25%\n",
      "Epoch 26/50, Loss: 0.3595, Accuracy: 84.25%\n",
      "Epoch 27/50, Loss: 0.3422, Accuracy: 84.38%\n",
      "Epoch 28/50, Loss: 0.3345, Accuracy: 85.25%\n",
      "Epoch 29/50, Loss: 0.3452, Accuracy: 83.50%\n",
      "Epoch 30/50, Loss: 0.3286, Accuracy: 84.75%\n",
      "Epoch 31/50, Loss: 0.3511, Accuracy: 84.38%\n",
      "Epoch 32/50, Loss: 0.3407, Accuracy: 85.88%\n",
      "Epoch 33/50, Loss: 0.3361, Accuracy: 86.00%\n",
      "Epoch 34/50, Loss: 0.3341, Accuracy: 85.62%\n",
      "Epoch 35/50, Loss: 0.3251, Accuracy: 85.88%\n",
      "Epoch 36/50, Loss: 0.3144, Accuracy: 87.00%\n",
      "Epoch 37/50, Loss: 0.3010, Accuracy: 88.75%\n",
      "Epoch 38/50, Loss: 0.3204, Accuracy: 86.38%\n",
      "Epoch 39/50, Loss: 0.3262, Accuracy: 84.88%\n",
      "Epoch 40/50, Loss: 0.3061, Accuracy: 87.25%\n",
      "Epoch 41/50, Loss: 0.3199, Accuracy: 86.12%\n",
      "Epoch 42/50, Loss: 0.2862, Accuracy: 88.00%\n",
      "Epoch 43/50, Loss: 0.2862, Accuracy: 88.25%\n",
      "Epoch 44/50, Loss: 0.3011, Accuracy: 86.38%\n",
      "Epoch 45/50, Loss: 0.3036, Accuracy: 88.00%\n",
      "Epoch 46/50, Loss: 0.2966, Accuracy: 88.88%\n",
      "Epoch 47/50, Loss: 0.3064, Accuracy: 87.38%\n",
      "Epoch 48/50, Loss: 0.2760, Accuracy: 89.12%\n",
      "Epoch 49/50, Loss: 0.2559, Accuracy: 90.00%\n",
      "Epoch 50/50, Loss: 0.2984, Accuracy: 86.25%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Definir el modelo\n",
    "model = YOLOv8nCls()  # tu modelo recreado\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 2. Definir la función de pérdida\n",
    "criterion = nn.CrossEntropyLoss()  # porque es clasificación\n",
    "\n",
    "# 3. Definir el optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 4. Bucle de entrenamiento\n",
    "num_epochs = 50  # ajusta según tu dataset\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # resetear gradientes\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {train_acc:.2f}%\"\n",
    "    )\n",
    "\n",
    "# 5. Guardar pesos entrenados\n",
    "torch.save(model.state_dict(), \"modelo_entrenado.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2078df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 80.60%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "val_acc = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40267add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.jpeg --> Predicción: bird\n",
      "11.jpg --> Predicción: bird\n",
      "12.jpg --> Predicción: bird\n",
      "13.jpg --> Predicción: no_bird\n",
      "14.jpg --> Predicción: no_bird\n",
      "16.jpg --> Predicción: no_bird\n",
      "3.jpg --> Predicción: no_bird\n",
      "349664072_639634714750284_7488197136792291295_n.jpg --> Predicción: bird\n",
      "4.jpg --> Predicción: bird\n",
      "5.jpg --> Predicción: bird\n",
      "6.jpeg --> Predicción: bird\n",
      "65a9d7da0d6bb119203b1c13.jpg --> Predicción: no_bird\n",
      "7.jpeg --> Predicción: bird\n",
      "8.jpg --> Predicción: bird\n",
      "9.jpg --> Predicción: bird\n",
      "images.jpg --> Predicción: no_bird\n",
      "images_1.jpeg --> Predicción: bird\n",
      "orig-1437426411440.jpg --> Predicción: bird\n",
      "pantanos-scaled.jpg --> Predicción: bird\n",
      "paseo-en-catamaran-insonoro.jpg --> Predicción: no_bird\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "folder = \"Images_test\"\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith((\".jpg\", \".png\", \".jpeg\")):  # filtra solo imágenes\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img = transform(img).unsqueeze(0).to(device)  # agrega batch dimension\n",
    "\n",
    "            output = model(img)\n",
    "            pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "            print(f\"{filename} --> Predicción: {train_dataset.classes[pred_class]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
