{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cab8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n-cls.pt\")\n",
    "\n",
    "model.train(data=\"dataset_split\", epochs=50, imgsz=224, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4ce5b9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Probando en imagen: 10.jpeg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\10.jpeg: 224x224 bird 1.00, no_bird 0.00, 41.4ms\n",
      "Speed: 38.9ms preprocess, 41.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 11.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\11.jpg: 224x224 bird 1.00, no_bird 0.00, 9.0ms\n",
      "Speed: 14.8ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 12.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\12.jpg: 224x224 bird 0.87, no_bird 0.13, 8.3ms\n",
      "Speed: 13.5ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 0.87\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 13.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\13.jpg: 224x224 bird 1.00, no_bird 0.00, 8.2ms\n",
      "Speed: 13.5ms preprocess, 8.2ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 14.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\14.jpg: 224x224 no_bird 1.00, bird 0.00, 7.7ms\n",
      "Speed: 8.2ms preprocess, 7.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "❌ No hay ave (confianza 1.00)\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 16.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\16.jpg: 224x224 no_bird 0.80, bird 0.20, 9.5ms\n",
      "Speed: 104.2ms preprocess, 9.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "❌ No hay ave (confianza 0.80)\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 3.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\3.jpg: 224x224 bird 0.54, no_bird 0.46, 8.3ms\n",
      "Speed: 4.3ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 0.54\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 349664072_639634714750284_7488197136792291295_n.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\349664072_639634714750284_7488197136792291295_n.jpg: 224x224 bird 1.00, no_bird 0.00, 9.0ms\n",
      "Speed: 16.5ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 4.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\4.jpg: 224x224 bird 1.00, no_bird 0.00, 8.3ms\n",
      "Speed: 13.6ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 5.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\5.jpg: 224x224 bird 1.00, no_bird 0.00, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 6.jpeg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\6.jpeg: 224x224 bird 1.00, no_bird 0.00, 9.8ms\n",
      "Speed: 4.0ms preprocess, 9.8ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 65a9d7da0d6bb119203b1c13.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\65a9d7da0d6bb119203b1c13.jpg: 224x224 bird 1.00, no_bird 0.00, 8.3ms\n",
      "Speed: 13.7ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 7.jpeg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\7.jpeg: 224x224 bird 1.00, no_bird 0.00, 9.4ms\n",
      "Speed: 3.4ms preprocess, 9.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 8.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\8.jpg: 224x224 bird 0.99, no_bird 0.01, 7.9ms\n",
      "Speed: 8.8ms preprocess, 7.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 0.99\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: 9.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\9.jpg: 224x224 bird 1.00, no_bird 0.00, 8.7ms\n",
      "Speed: 7.2ms preprocess, 8.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: images.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\images.jpg: 224x224 no_bird 1.00, bird 0.00, 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "❌ No hay ave (confianza 1.00)\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: images_1.jpeg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\images_1.jpeg: 224x224 bird 1.00, no_bird 0.00, 8.8ms\n",
      "Speed: 1.7ms preprocess, 8.8ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: orig-1437426411440.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\orig-1437426411440.jpg: 224x224 bird 1.00, no_bird 0.00, 8.8ms\n",
      "Speed: 2.7ms preprocess, 8.8ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: pantanos-scaled.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\pantanos-scaled.jpg: 224x224 bird 1.00, no_bird 0.00, 7.8ms\n",
      "Speed: 24.6ms preprocess, 7.8ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 1.00\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: paseo-en-catamaran-insonoro.jpg\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\paseo-en-catamaran-insonoro.jpg: 224x224 no_bird 0.98, bird 0.02, 8.4ms\n",
      "Speed: 4.3ms preprocess, 8.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "❌ No hay ave (confianza 0.98)\n",
      "----------------------------------------\n",
      "🔍 Probando en imagen: reducIMG_4616.JPG\n",
      "\n",
      "image 1/1 d:\\2025\\Tesis\\ML_PYNQ\\Images_test\\reducIMG_4616.JPG: 224x224 bird 0.83, no_bird 0.17, 9.3ms\n",
      "Speed: 3.9ms preprocess, 9.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "✅ Hay un ave con confianza 0.83\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Carpeta con imágenes\n",
    "IMAGE_FOLDER = \"Images_test\"\n",
    "model = YOLO(\"runs/classify/train/weights/best.pt\")\n",
    "\n",
    "# Obtener todas las imágenes de la carpeta (jpg, png)\n",
    "imagenes = [\n",
    "    f for f in os.listdir(IMAGE_FOLDER) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "]\n",
    "\n",
    "for img_name in imagenes:\n",
    "    img_path = os.path.join(IMAGE_FOLDER, img_name)\n",
    "    print(\"🔍 Probando en imagen:\", img_name)\n",
    "\n",
    "    results = model.predict(source=img_path)\n",
    "\n",
    "    for r in results:\n",
    "        # Obtener la clase y la confianza top1\n",
    "        clase = r.names[r.probs.top1]\n",
    "        conf = r.probs.top1conf.item()\n",
    "\n",
    "        if clase == \"bird\":\n",
    "            print(f\"✅ Hay un ave con confianza {conf:.2f}\")\n",
    "        else:\n",
    "            print(f\"❌ No hay ave (confianza {conf:.2f})\")\n",
    "\n",
    "    print(\"-\" * 40)  # Separador entre imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec97f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n-cls summary: 56 layers, 1,440,850 parameters, 0 gradients, 3.4 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(56, 1440850, 0, 3.3619968)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO(\"runs/classify/train/weights/best.pt\")\n",
    "model.info()  # Mostrar información del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b524796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv.weight torch.Size([16, 3, 3, 3])\n",
      "model.0.bn.weight torch.Size([16])\n",
      "model.0.bn.bias torch.Size([16])\n",
      "model.0.bn.running_mean torch.Size([16])\n",
      "model.0.bn.running_var torch.Size([16])\n",
      "model.0.bn.num_batches_tracked torch.Size([])\n",
      "model.1.conv.weight torch.Size([32, 16, 3, 3])\n",
      "model.1.bn.weight torch.Size([32])\n",
      "model.1.bn.bias torch.Size([32])\n",
      "model.1.bn.running_mean torch.Size([32])\n",
      "model.1.bn.running_var torch.Size([32])\n",
      "model.1.bn.num_batches_tracked torch.Size([])\n",
      "model.2.cv1.conv.weight torch.Size([32, 32, 1, 1])\n",
      "model.2.cv1.bn.weight torch.Size([32])\n",
      "model.2.cv1.bn.bias torch.Size([32])\n",
      "model.2.cv1.bn.running_mean torch.Size([32])\n",
      "model.2.cv1.bn.running_var torch.Size([32])\n",
      "model.2.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.2.cv2.conv.weight torch.Size([32, 48, 1, 1])\n",
      "model.2.cv2.bn.weight torch.Size([32])\n",
      "model.2.cv2.bn.bias torch.Size([32])\n",
      "model.2.cv2.bn.running_mean torch.Size([32])\n",
      "model.2.cv2.bn.running_var torch.Size([32])\n",
      "model.2.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.2.m.0.cv1.conv.weight torch.Size([16, 16, 3, 3])\n",
      "model.2.m.0.cv1.bn.weight torch.Size([16])\n",
      "model.2.m.0.cv1.bn.bias torch.Size([16])\n",
      "model.2.m.0.cv1.bn.running_mean torch.Size([16])\n",
      "model.2.m.0.cv1.bn.running_var torch.Size([16])\n",
      "model.2.m.0.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.2.m.0.cv2.conv.weight torch.Size([16, 16, 3, 3])\n",
      "model.2.m.0.cv2.bn.weight torch.Size([16])\n",
      "model.2.m.0.cv2.bn.bias torch.Size([16])\n",
      "model.2.m.0.cv2.bn.running_mean torch.Size([16])\n",
      "model.2.m.0.cv2.bn.running_var torch.Size([16])\n",
      "model.2.m.0.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.3.conv.weight torch.Size([64, 32, 3, 3])\n",
      "model.3.bn.weight torch.Size([64])\n",
      "model.3.bn.bias torch.Size([64])\n",
      "model.3.bn.running_mean torch.Size([64])\n",
      "model.3.bn.running_var torch.Size([64])\n",
      "model.3.bn.num_batches_tracked torch.Size([])\n",
      "model.4.cv1.conv.weight torch.Size([64, 64, 1, 1])\n",
      "model.4.cv1.bn.weight torch.Size([64])\n",
      "model.4.cv1.bn.bias torch.Size([64])\n",
      "model.4.cv1.bn.running_mean torch.Size([64])\n",
      "model.4.cv1.bn.running_var torch.Size([64])\n",
      "model.4.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.4.cv2.conv.weight torch.Size([64, 128, 1, 1])\n",
      "model.4.cv2.bn.weight torch.Size([64])\n",
      "model.4.cv2.bn.bias torch.Size([64])\n",
      "model.4.cv2.bn.running_mean torch.Size([64])\n",
      "model.4.cv2.bn.running_var torch.Size([64])\n",
      "model.4.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.4.m.0.cv1.conv.weight torch.Size([32, 32, 3, 3])\n",
      "model.4.m.0.cv1.bn.weight torch.Size([32])\n",
      "model.4.m.0.cv1.bn.bias torch.Size([32])\n",
      "model.4.m.0.cv1.bn.running_mean torch.Size([32])\n",
      "model.4.m.0.cv1.bn.running_var torch.Size([32])\n",
      "model.4.m.0.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.4.m.0.cv2.conv.weight torch.Size([32, 32, 3, 3])\n",
      "model.4.m.0.cv2.bn.weight torch.Size([32])\n",
      "model.4.m.0.cv2.bn.bias torch.Size([32])\n",
      "model.4.m.0.cv2.bn.running_mean torch.Size([32])\n",
      "model.4.m.0.cv2.bn.running_var torch.Size([32])\n",
      "model.4.m.0.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.4.m.1.cv1.conv.weight torch.Size([32, 32, 3, 3])\n",
      "model.4.m.1.cv1.bn.weight torch.Size([32])\n",
      "model.4.m.1.cv1.bn.bias torch.Size([32])\n",
      "model.4.m.1.cv1.bn.running_mean torch.Size([32])\n",
      "model.4.m.1.cv1.bn.running_var torch.Size([32])\n",
      "model.4.m.1.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.4.m.1.cv2.conv.weight torch.Size([32, 32, 3, 3])\n",
      "model.4.m.1.cv2.bn.weight torch.Size([32])\n",
      "model.4.m.1.cv2.bn.bias torch.Size([32])\n",
      "model.4.m.1.cv2.bn.running_mean torch.Size([32])\n",
      "model.4.m.1.cv2.bn.running_var torch.Size([32])\n",
      "model.4.m.1.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.5.conv.weight torch.Size([128, 64, 3, 3])\n",
      "model.5.bn.weight torch.Size([128])\n",
      "model.5.bn.bias torch.Size([128])\n",
      "model.5.bn.running_mean torch.Size([128])\n",
      "model.5.bn.running_var torch.Size([128])\n",
      "model.5.bn.num_batches_tracked torch.Size([])\n",
      "model.6.cv1.conv.weight torch.Size([128, 128, 1, 1])\n",
      "model.6.cv1.bn.weight torch.Size([128])\n",
      "model.6.cv1.bn.bias torch.Size([128])\n",
      "model.6.cv1.bn.running_mean torch.Size([128])\n",
      "model.6.cv1.bn.running_var torch.Size([128])\n",
      "model.6.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.6.cv2.conv.weight torch.Size([128, 256, 1, 1])\n",
      "model.6.cv2.bn.weight torch.Size([128])\n",
      "model.6.cv2.bn.bias torch.Size([128])\n",
      "model.6.cv2.bn.running_mean torch.Size([128])\n",
      "model.6.cv2.bn.running_var torch.Size([128])\n",
      "model.6.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.6.m.0.cv1.conv.weight torch.Size([64, 64, 3, 3])\n",
      "model.6.m.0.cv1.bn.weight torch.Size([64])\n",
      "model.6.m.0.cv1.bn.bias torch.Size([64])\n",
      "model.6.m.0.cv1.bn.running_mean torch.Size([64])\n",
      "model.6.m.0.cv1.bn.running_var torch.Size([64])\n",
      "model.6.m.0.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.6.m.0.cv2.conv.weight torch.Size([64, 64, 3, 3])\n",
      "model.6.m.0.cv2.bn.weight torch.Size([64])\n",
      "model.6.m.0.cv2.bn.bias torch.Size([64])\n",
      "model.6.m.0.cv2.bn.running_mean torch.Size([64])\n",
      "model.6.m.0.cv2.bn.running_var torch.Size([64])\n",
      "model.6.m.0.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.6.m.1.cv1.conv.weight torch.Size([64, 64, 3, 3])\n",
      "model.6.m.1.cv1.bn.weight torch.Size([64])\n",
      "model.6.m.1.cv1.bn.bias torch.Size([64])\n",
      "model.6.m.1.cv1.bn.running_mean torch.Size([64])\n",
      "model.6.m.1.cv1.bn.running_var torch.Size([64])\n",
      "model.6.m.1.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.6.m.1.cv2.conv.weight torch.Size([64, 64, 3, 3])\n",
      "model.6.m.1.cv2.bn.weight torch.Size([64])\n",
      "model.6.m.1.cv2.bn.bias torch.Size([64])\n",
      "model.6.m.1.cv2.bn.running_mean torch.Size([64])\n",
      "model.6.m.1.cv2.bn.running_var torch.Size([64])\n",
      "model.6.m.1.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.7.conv.weight torch.Size([256, 128, 3, 3])\n",
      "model.7.bn.weight torch.Size([256])\n",
      "model.7.bn.bias torch.Size([256])\n",
      "model.7.bn.running_mean torch.Size([256])\n",
      "model.7.bn.running_var torch.Size([256])\n",
      "model.7.bn.num_batches_tracked torch.Size([])\n",
      "model.8.cv1.conv.weight torch.Size([256, 256, 1, 1])\n",
      "model.8.cv1.bn.weight torch.Size([256])\n",
      "model.8.cv1.bn.bias torch.Size([256])\n",
      "model.8.cv1.bn.running_mean torch.Size([256])\n",
      "model.8.cv1.bn.running_var torch.Size([256])\n",
      "model.8.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.8.cv2.conv.weight torch.Size([256, 384, 1, 1])\n",
      "model.8.cv2.bn.weight torch.Size([256])\n",
      "model.8.cv2.bn.bias torch.Size([256])\n",
      "model.8.cv2.bn.running_mean torch.Size([256])\n",
      "model.8.cv2.bn.running_var torch.Size([256])\n",
      "model.8.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.8.m.0.cv1.conv.weight torch.Size([128, 128, 3, 3])\n",
      "model.8.m.0.cv1.bn.weight torch.Size([128])\n",
      "model.8.m.0.cv1.bn.bias torch.Size([128])\n",
      "model.8.m.0.cv1.bn.running_mean torch.Size([128])\n",
      "model.8.m.0.cv1.bn.running_var torch.Size([128])\n",
      "model.8.m.0.cv1.bn.num_batches_tracked torch.Size([])\n",
      "model.8.m.0.cv2.conv.weight torch.Size([128, 128, 3, 3])\n",
      "model.8.m.0.cv2.bn.weight torch.Size([128])\n",
      "model.8.m.0.cv2.bn.bias torch.Size([128])\n",
      "model.8.m.0.cv2.bn.running_mean torch.Size([128])\n",
      "model.8.m.0.cv2.bn.running_var torch.Size([128])\n",
      "model.8.m.0.cv2.bn.num_batches_tracked torch.Size([])\n",
      "model.9.conv.conv.weight torch.Size([1280, 256, 1, 1])\n",
      "model.9.conv.bn.weight torch.Size([1280])\n",
      "model.9.conv.bn.bias torch.Size([1280])\n",
      "model.9.conv.bn.running_mean torch.Size([1280])\n",
      "model.9.conv.bn.running_var torch.Size([1280])\n",
      "model.9.conv.bn.num_batches_tracked torch.Size([])\n",
      "model.9.linear.weight torch.Size([2, 1280])\n",
      "model.9.linear.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"runs/classify/train/weights/best.pt\")\n",
    "pt_model = model.model  \n",
    "\n",
    "for name, param in pt_model.state_dict().items():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a4abe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase predicha: 1\n",
      "Confianza: 0.7302632331848145\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Imagen de prueba\n",
    "img_path = \"Images_test/paseo-en-catamaran-insonoro.jpg\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Transformaciones que espera YOLO\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)), transforms.ToTensor()]  # Tamaño típico de entrada\n",
    ")\n",
    "\n",
    "input_tensor = transform(img).unsqueeze(0)  # Añadir batch dimension\n",
    "# Inferencia\n",
    "pt_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = pt_model(input_tensor)  # output es un tuple: (logits, ...)\n",
    "\n",
    "# En YOLOv8, el primer elemento del tuple suele ser logits\n",
    "logits = output[0]\n",
    "\n",
    "# Ahora sí aplicas softmax\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "top1_prob, top1_class = torch.max(probs, dim=1)\n",
    "\n",
    "print(\"Clase predicha:\", top1_class.item())\n",
    "print(\"Confianza:\", top1_prob.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2dc9270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationModel(\n",
      "  (model): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (2): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (4): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-1): 2 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Conv(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (6): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0-1): 2 x Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): Conv(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (8): C2f(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): Classify(\n",
      "      (conv): Conv(\n",
      "        (conv): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (drop): Dropout(p=0.0, inplace=True)\n",
      "      (linear): Linear(in_features=1280, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246cc9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"Images_test/paseo-en-catamaran-insonoro.jpg\")\n",
    "print(img.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fbaa53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)        \n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 100)  # Ajusta según el tamaño de entrada\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(100, 2) # 2 clases: ave y no ave\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5bb5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformaciones: ajustar tamaño y normalizar\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3),\n",
    "        transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "val_dataset = ImageFolder(root=\"dataset_split/val\", transform=val_transform)\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root=\"dataset_split/train\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "81026415",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CustomCNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Para clasificación multiclase\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c89ff6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 0.6842 - Acc: 52.88%\n",
      "Epoch 2/30 - Loss: 0.6557 - Acc: 63.62%\n",
      "Epoch 3/30 - Loss: 0.5953 - Acc: 70.38%\n",
      "Epoch 4/30 - Loss: 0.5911 - Acc: 68.38%\n",
      "Epoch 5/30 - Loss: 0.5354 - Acc: 73.88%\n",
      "Epoch 6/30 - Loss: 0.5505 - Acc: 73.25%\n",
      "Epoch 7/30 - Loss: 0.5504 - Acc: 76.75%\n",
      "Epoch 8/30 - Loss: 0.5485 - Acc: 75.25%\n",
      "Epoch 9/30 - Loss: 0.5129 - Acc: 76.50%\n",
      "Epoch 10/30 - Loss: 0.5067 - Acc: 74.88%\n",
      "Epoch 11/30 - Loss: 0.5057 - Acc: 76.62%\n",
      "Epoch 12/30 - Loss: 0.4708 - Acc: 77.62%\n",
      "Epoch 13/30 - Loss: 0.4845 - Acc: 76.62%\n",
      "Epoch 14/30 - Loss: 0.4774 - Acc: 78.88%\n",
      "Epoch 15/30 - Loss: 0.4553 - Acc: 79.50%\n",
      "Epoch 16/30 - Loss: 0.4535 - Acc: 79.88%\n",
      "Epoch 17/30 - Loss: 0.4496 - Acc: 79.75%\n",
      "Epoch 18/30 - Loss: 0.4326 - Acc: 80.62%\n",
      "Epoch 19/30 - Loss: 0.4569 - Acc: 79.75%\n",
      "Epoch 20/30 - Loss: 0.4239 - Acc: 81.75%\n",
      "Epoch 21/30 - Loss: 0.4288 - Acc: 80.88%\n",
      "Epoch 22/30 - Loss: 0.4350 - Acc: 80.50%\n",
      "Epoch 23/30 - Loss: 0.4195 - Acc: 81.50%\n",
      "Epoch 24/30 - Loss: 0.4263 - Acc: 81.12%\n",
      "Epoch 25/30 - Loss: 0.4067 - Acc: 81.50%\n",
      "Epoch 26/30 - Loss: 0.3953 - Acc: 84.12%\n",
      "Epoch 27/30 - Loss: 0.3864 - Acc: 84.00%\n",
      "Epoch 28/30 - Loss: 0.3777 - Acc: 83.25%\n",
      "Epoch 29/30 - Loss: 0.4025 - Acc: 81.62%\n",
      "Epoch 30/30 - Loss: 0.3824 - Acc: 82.50%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_loader):.4f} - Acc: {100*correct/total:.2f}%\"\n",
    "    )\n",
    "\n",
    "    scheduler.step(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a9b46188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 78.11%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "\n",
    "val_acc = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea710446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.jpeg --> Predicción: bird\n",
      "11.jpg --> Predicción: no_bird\n",
      "12.jpg --> Predicción: bird\n",
      "13.jpg --> Predicción: bird\n",
      "14.jpg --> Predicción: bird\n",
      "16.jpg --> Predicción: no_bird\n",
      "17.jpg --> Predicción: no_bird\n",
      "3.jpg --> Predicción: no_bird\n",
      "349664072_639634714750284_7488197136792291295_n.jpg --> Predicción: bird\n",
      "4.jpg --> Predicción: bird\n",
      "5.jpg --> Predicción: no_bird\n",
      "6.jpeg --> Predicción: bird\n",
      "65a9d7da0d6bb119203b1c13.jpg --> Predicción: bird\n",
      "7.jpeg --> Predicción: bird\n",
      "8.jpg --> Predicción: bird\n",
      "9.jpg --> Predicción: bird\n",
      "images.jpg --> Predicción: no_bird\n",
      "images_1.jpeg --> Predicción: bird\n",
      "orig-1437426411440.jpg --> Predicción: bird\n",
      "pantanos-scaled.jpg --> Predicción: bird\n",
      "paseo-en-catamaran-insonoro.jpg --> Predicción: no_bird\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "folder = \"Images_test\"\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith((\".jpg\", \".png\", \".jpeg\")):  # filtra solo imágenes\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img = transform(img).unsqueeze(0).to(device)  # agrega batch dimension\n",
    "\n",
    "            output = model(img)\n",
    "            pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "            print(f\"{filename} --> Predicción: {train_dataset.classes[pred_class]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae56f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
