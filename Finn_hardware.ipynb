{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "60491b51-babd-4b8f-938d-51ccdf767a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/finn_dev_parasyte\n"
     ]
    }
   ],
   "source": [
    "user_name = \"parasyte\" # REPLACE THIS WITH YOUR HOST MACHINE USER NAME \n",
    "root_dir = f\"/tmp/finn_dev_{user_name}\"\n",
    "print(root_dir)\n",
    "# get onnx model from the last NOTEBOOK\n",
    "filename = root_dir + \"/ready_finn.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "17b4cbe8-7d00-48c4-bac7-8717a3f1fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showSrc, showInNetron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c0ce7189-7722-4cad-ba4b-91e204be6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showInNetron(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "384acc5f-5fac-42b1-8e71-06e69d709516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class InferShapes(Transformation):\n",
      "    \"\"\"Ensure every tensor in the model has a specified shape (ValueInfo).\"\"\"\n",
      "\n",
      "    def apply(self, model):\n",
      "        # hide your riches!\n",
      "        hidden_ops = _hide_finn_ops(model)\n",
      "        # call regular ONNX shape inference\n",
      "        model = ModelWrapper(si.infer_shapes(model.model))\n",
      "        # bring back hidden ops\n",
      "        _restore_finn_ops(model, hidden_ops)\n",
      "        return (model, False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "model = ModelWrapper(filename)\n",
    "\n",
    "# TIDY UP\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "model.save(root_dir + \"/tidy.onnx\")\n",
    "\n",
    "showSrc(InferShapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d2ee3e35-2ae5-4ed2-913b-82585f6af619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showInNetron(root_dir + \"/tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8001b773-ab70-43ec-a2cf-3bdb90572dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "import torch\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "# PRE PROC : NONE\n",
    "model = ModelWrapper(root_dir + \"/tidy.onnx\")\n",
    "\n",
    "#=========================================================\n",
    "# Here you could add some proprocessing if you need some\n",
    "#=========================================================\n",
    "\n",
    "# add input annotation: UINT8 is what we will feed the model during inference\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "\n",
    "model.save(root_dir + \"/full_preproc.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "deb5f8cd-eb9c-4820-a2c1-53d12b58d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showInNetron(root_dir + \"/full_preproc.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b09bd025-7bfb-428e-af47-c73be958bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "# POST PROC\n",
    "\n",
    "# insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = root_dir + \"/pre_post.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)\n",
    "\n",
    "# showInNetron(chkpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b4822010-2f2f-4d7d-94cf-ba897eafc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from finn.transformation.streamline import Streamline\n",
    "# we can see the list of apllied transformations here : showSrc(Streamline)\n",
    "from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/pre_post.onnx\")\n",
    "# STREAMLINE\n",
    "model = model.transform(Streamline())\n",
    "model.save(root_dir + \"/streamlined.onnx\")\n",
    "# showInNetron(root_dir + \"/streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7c610224-eded-4ad2-b7f9-dd49c3a2668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/streamlined.onnx\")\n",
    "\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(RoundAndClipThresholds())\n",
    "\n",
    "# bit of tidy-up\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(root_dir + \"/streamlined_merged_and_ready.onnx\")\n",
    "# showInNetron(root_dir + \"/streamlined_merged_and_ready.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a2a3b0c4-6592-4cc4-83e1-069c98033923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "# TO HW LAYERS\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/streamlined_merged_and_ready.onnx\")\n",
    "model = model.transform(to_hw.InferLabelSelectLayer())\n",
    "model = model.transform(to_hw.InferChannelwiseLinearLayer())\n",
    "model = model.transform(to_hw.InferQuantizedMatrixVectorActivation())\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model.save(root_dir + \"/hw.onnx\")\n",
    "# showInNetron(root_dir + \"/hw.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5877c229-23ed-43f5-931e-b634fa5ab35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipos de nodos encontrados en hw.onnx:\n",
      " {'Reshape', 'Mul', 'MVAU', 'TopK', 'Add'}\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "hw_model = ModelWrapper(root_dir + \"/hw.onnx\")\n",
    "\n",
    "# listar todos los tipos de nodos que contiene\n",
    "op_types = set([n.op_type for n in hw_model.graph.node])\n",
    "print(\"Tipos de nodos encontrados en hw.onnx:\\n\", op_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c0893de0-54de-401b-ae37-307f8240a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/hw.onnx\")\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(root_dir + \"/df_part.onnx\")\n",
    "# showInNetron(root_dir + \"/df_part.onnx\")\n",
    "\n",
    "# We see that our entire model is now part of a child model in \"StreamingDataflowPartition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7ad12209-d568-4dfc-aae7-a9ff5c8dc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.custom_op.registry import getCustomOp\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# showInNetron(dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9ccd06f3-37fc-4316-81e1-27d0dda480be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Ultra96', 'Ultra96-V2', 'Pynq-Z1', 'Pynq-Z2', 'ZCU102', 'ZCU104', 'ZCU111', 'RFSoC2x2', 'RFSoC4x2', 'KV260_SOM'])\n"
     ]
    }
   ],
   "source": [
    "# print the names of the supported PYNQ boards\n",
    "from finn.util.basic import pynq_part_map\n",
    "print(pynq_part_map.keys())\n",
    "\n",
    "# change this if you have a different PYNQ board, see list above\n",
    "pynq_board = \"Pynq-Z2\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "99855ce7-b704-4660-aabc-3fdaab8baa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class SpecializeLayers(Transformation):\n",
      "    \"\"\"Specialize all layers to either HLS or RTL variants\"\"\"\n",
      "\n",
      "    def __init__(self, fpgapart):\n",
      "        super().__init__()\n",
      "        self.fpgapart = fpgapart\n",
      "\n",
      "    def apply(self, model):\n",
      "        graph = model.graph\n",
      "        node_ind = 0\n",
      "        graph_modified = False\n",
      "        for node in graph.node:\n",
      "            # Skip nodes that are not hw layers\n",
      "            if not node.domain == \"finn.custom_op.fpgadataflow\":\n",
      "                continue\n",
      "            node_ind += 1\n",
      "            impl_style = _determine_impl_style(node, self.fpgapart, model)\n",
      "            optype = node.op_type + \"_\" + impl_style\n",
      "\n",
      "            new_node = helper.make_node(\n",
      "                optype,\n",
      "                node.input,\n",
      "                node.output,\n",
      "                domain=\"finn.custom_op.fpgadataflow.\" + impl_style,\n",
      "            )\n",
      "            # add all attributes\n",
      "            for attribute in node.attribute:\n",
      "                if attribute.name != \"preferred_impl_style\":\n",
      "                    new_node.attribute.append(attribute)\n",
      "            graph.node.insert(node_ind, new_node)\n",
      "            # remove old nodes\n",
      "            graph.node.remove(node)\n",
      "            graph_modified = True\n",
      "        return (model, graph_modified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "model = ModelWrapper(dataflow_model_filename)\n",
    "\n",
    "model = model.transform(SpecializeLayers(fpga_part))\n",
    "\n",
    "showSrc(SpecializeLayers)\n",
    "\n",
    "model.save(root_dir + \"/to_hw_conv.onnx\")\n",
    "# showInNetron(root_dir + \"/to_hw_conv.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b4b20b82-ce89-4b82-9abb-a5156bdaccd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomOp wrapper is of class MVAU_hls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PE': ('i', True, 0),\n",
       " 'SIMD': ('i', True, 0),\n",
       " 'MW': ('i', True, 0),\n",
       " 'MH': ('i', True, 0),\n",
       " 'resType': ('s', False, 'auto', {'auto', 'dsp', 'lut'}),\n",
       " 'ActVal': ('i', False, 0),\n",
       " 'inputDataType': ('s', True, ''),\n",
       " 'weightDataType': ('s', True, ''),\n",
       " 'outputDataType': ('s', True, ''),\n",
       " 'accDataType': ('s', False, 'INT32'),\n",
       " 'binaryXnorMode': ('i', False, 0, {0, 1}),\n",
       " 'noActivation': ('i', False, 0, {0, 1}),\n",
       " 'numInputVectors': ('ints', False, [1]),\n",
       " 'mem_mode': ('s',\n",
       "  False,\n",
       "  'internal_decoupled',\n",
       "  {'external', 'internal_decoupled', 'internal_embedded'}),\n",
       " 'ram_style': ('s', False, 'auto', {'auto', 'block', 'distributed', 'ultra'}),\n",
       " 'ram_style_thresholds': ('s',\n",
       "  False,\n",
       "  'auto',\n",
       "  {'auto', 'block', 'distributed'}),\n",
       " 'runtime_writeable_weights': ('i', False, 0, {0, 1}),\n",
       " 'backend': ('s', True, 'fpgadataflow'),\n",
       " 'preferred_impl_style': ('s', False, '', {'', 'hls', 'rtl'}),\n",
       " 'code_gen_dir_ipgen': ('s', False, ''),\n",
       " 'ipgen_path': ('s', False, ''),\n",
       " 'ip_path': ('s', False, ''),\n",
       " 'ip_vlnv': ('s', False, ''),\n",
       " 'exec_mode': ('s', False, '', {'', 'cppsim', 'rtlsim'}),\n",
       " 'cycles_rtlsim': ('i', False, 0),\n",
       " 'cycles_estimate': ('i', False, 0),\n",
       " 'rtlsim_trace': ('s', False, ''),\n",
       " 'res_estimate': ('s', False, ''),\n",
       " 'res_synth': ('s', False, ''),\n",
       " 'rtlsim_so': ('s', False, ''),\n",
       " 'slr': ('i', False, -1),\n",
       " 'mem_port': ('s', False, ''),\n",
       " 'partition_id': ('i', False, 0),\n",
       " 'device_id': ('i', False, 0),\n",
       " 'inFIFODepths': ('ints', False, [2]),\n",
       " 'outFIFODepths': ('ints', False, [2]),\n",
       " 'output_hook': ('s', False, ''),\n",
       " 'io_chrc_in': ('t', False, array([], dtype=int32)),\n",
       " 'io_chrc_out': ('t', False, array([], dtype=int32)),\n",
       " 'io_chrc_period': ('i', False, 0),\n",
       " 'io_chrc_pads_in': ('ints', False, []),\n",
       " 'io_chrc_pads_out': ('ints', False, []),\n",
       " 'code_gen_dir_cppsim': ('s', False, ''),\n",
       " 'executable_path': ('s', False, ''),\n",
       " 'res_hls': ('s', False, '')}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(root_dir + \"/to_hw_conv.onnx\")\n",
    "\n",
    "fc0 = model.graph.node[0] #1st MVAU\n",
    "fc0w = getCustomOp(fc0)\n",
    "\n",
    "print(\"CustomOp wrapper is of class \" + fc0w.__class__.__name__)\n",
    "\n",
    "fc0w.get_nodeattr_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f515cce7-22ff-4bef-9536-28e2ae51c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMD: 16\n",
      "PE: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fc0 = model.graph.node[0] #1st MVAU\n",
    "fc0w = getCustomOp(fc0)\n",
    "fc0w.set_nodeattr(\"SIMD\",16)\n",
    "fc0w.set_nodeattr(\"PE\",8)\n",
    "\n",
    "fc0w = getCustomOp(fc0)\n",
    "print(\"SIMD:\", fc0w.get_nodeattr(\"SIMD\"))\n",
    "print(\"PE:\", fc0w.get_nodeattr(\"PE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "097a4a7c-a7a6-4f91-b58a-1a7c35d195fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMD: 3\n",
      "PE: 1\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.set_folding import SetFolding\n",
    "\n",
    "TARGET_FPS = 100\n",
    "target_clk_cycles_per_frame = 1/(TARGET_FPS * 10e-9)\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/to_hw_conv.onnx\")\n",
    "# Actual method SetFolding that will automatically modify the folding settings\n",
    "model.transform(SetFolding(target_cycles_per_frame=target_clk_cycles_per_frame, mvau_wwidth_max=64, two_pass_relaxation=False))\n",
    "\n",
    "fc0 = model.graph.node[0] #1st MVAU\n",
    "fc0w = getCustomOp(fc0)\n",
    "fc0w.set_nodeattr(\"SIMD\",3)\n",
    "fc0w.set_nodeattr(\"PE\",1)\n",
    "\n",
    "fc0w = getCustomOp(fc0)\n",
    "print(\"SIMD:\", fc0w.get_nodeattr(\"SIMD\"))\n",
    "print(\"PE:\", fc0w.get_nodeattr(\"PE\"))\n",
    "model.save(root_dir + \"/to_hw_folded.onnx\")\n",
    "# showInNetron(root_dir + \"/to_hw_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e2ed9523-5375-49a2-867f-c87d0f85d8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running step: step_qonnx_to_finn [1/10]\n",
      "Running step: step_tidy_up [2/10]\n",
      "Running step: step_streamline [3/10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n",
      "Building dataflow accelerator from /tmp/finn_dev_parasyte/to_hw_folded.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_parasyte\n",
      "Final outputs will be generated in output_estimates_only\n",
      "Build log is at output_estimates_only/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/10]\n",
      "Running step: step_tidy_up [2/10]\n",
      "Running step: step_streamline [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running step: step_convert_to_hw [4/10]\n",
      "Running step: step_create_dataflow_partition [5/10]\n",
      "Running step: step_specialize_layers [6/10]\n",
      "Running step: step_target_fps_parallelization [7/10]\n",
      "Running step: step_apply_folding_config [8/10]\n",
      "Running step: step_minimize_bit_width [9/10]\n",
      "Running step: step_generate_estimate_reports [10/10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step: step_convert_to_hw [4/10]\n",
      "Running step: step_create_dataflow_partition [5/10]\n",
      "Running step: step_specialize_layers [6/10]\n",
      "Running step: step_target_fps_parallelization [7/10]\n",
      "Running step: step_apply_folding_config [8/10]\n",
      "Running step: step_minimize_bit_width [9/10]\n",
      "Running step: step_generate_estimate_reports [10/10]\n",
      "Completed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ESTIMATE\n",
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_file = root_dir + \"/to_hw_folded.onnx\"\n",
    "\n",
    "estimates_output_dir = \"output_estimates_only\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(estimates_output_dir):\n",
    "    shutil.rmtree(estimates_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "\n",
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 64,\n",
    "    target_fps          = 100,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xc7z020clg400-1\",\n",
    "    steps               = build_cfg.estimate_only_dataflow_steps,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")\n",
    "\n",
    "build.build_dataflow_cfg(model_file, cfg_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "baf52b3e-d94d-4033-bae0-79f093b8e717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"critical_path_cycles\": 403520,\n",
      "  \"max_cycles\": 393216,\n",
      "  \"max_cycles_node_name\": \"MVAU_hls_0\",\n",
      "  \"estimated_throughput_fps\": 254.31315104166666,\n",
      "  \"estimated_latency_ns\": 4035200.0\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {estimates_output_dir}/report/estimate_network_performance.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4e5b932b-c093-472a-a58e-e55a8d10766f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MVAU_hls_0': 393216,\n",
       " 'MVAU_hls_1': 8192,\n",
       " 'MVAU_hls_2': 2048,\n",
       " 'MVAU_rtl_0': 64}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def read_json_dict(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        ret = json.load(f)\n",
    "    return ret\n",
    "\n",
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_cycles.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "35fb324b-9f6f-496e-9256-6b4d3e72d4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MVAU_hls_0': {'BRAM_18K': 96,\n",
       "  'BRAM_efficiency': 0.8888888888888888,\n",
       "  'LUT': 375,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_hls_1': {'BRAM_18K': 4,\n",
       "  'BRAM_efficiency': 0.8888888888888888,\n",
       "  'LUT': 371,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_hls_2': {'BRAM_18K': 1,\n",
       "  'BRAM_efficiency': 0.8888888888888888,\n",
       "  'LUT': 370,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'MVAU_rtl_0': {'BRAM_18K': 1,\n",
       "  'BRAM_efficiency': 0.027777777777777776,\n",
       "  'LUT': 0,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 1},\n",
       " 'total': {'BRAM_18K': 102.0, 'LUT': 1116.0, 'URAM': 0.0, 'DSP': 1.0}}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_resources.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5f41538a-53be-45b7-9be9-10332695e90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parasyte/finn/src/finn/transformation/fpgadataflow/floorplan.py:107: UserWarning: 6 nodes have no entry in the provided floorplan, SLR was set to -1\n",
      "  warnings.warn(\n",
      "/home/parasyte/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:234: UserWarning: Input FIFO for IODMA_hls_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/parasyte/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:294: UserWarning: Output FIFO for MVAU_rtl_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/parasyte/finn/src/finn/transformation/fpgadataflow/create_stitched_ip.py:290: UserWarning: First node is not StreamingFIFO or IODMA.\n",
      "                You may experience incorrect stitched-IP rtlsim or hardware\n",
      "                behavior. It is strongly recommended to insert FIFOs prior to\n",
      "                calling CreateStitchedIP.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Actual hardware build, ZYNQ BUILD\n",
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "\n",
    "model = ModelWrapper(root_dir + \"/to_hw_folded.onnx\")\n",
    "model = model.transform(ZynqBuild(platform = pynq_board, period_ns = target_clk_ns,partition_model_dir=\"./test\",enable_debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5fbc1b19-387c-40f3-b115-0868364a6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save( \"post_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8b750e2a-287a-4612-916d-0441166cfcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Serving 'post_synth.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbe55263ee0>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"post_synth.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e6133df3-ce21-4dd6-af7b-9e3542a4616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(\"post_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bac614a3-85b1-4ebd-92a8-dfad14154174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[key: \"floorplan_json\"\n",
       "value: \"/tmp/finn_dev_parasyte/vitis_floorplan_gskc26cu/floorplan.json\"\n",
       ", key: \"vivado_pynq_proj\"\n",
       "value: \"/tmp/finn_dev_parasyte/vivado_zynq_proj_icavrp1d\"\n",
       ", key: \"bitfile\"\n",
       "value: \"/tmp/finn_dev_parasyte/vivado_zynq_proj_icavrp1d/resizer.bit\"\n",
       ", key: \"hw_handoff\"\n",
       "value: \"/tmp/finn_dev_parasyte/vivado_zynq_proj_icavrp1d/resizer.hwh\"\n",
       ", key: \"vivado_synth_rpt\"\n",
       "value: \"/tmp/finn_dev_parasyte/vivado_zynq_proj_icavrp1d/synth_report.xml\"\n",
       ", key: \"platform\"\n",
       "value: \"zynq-iodma\"\n",
       "]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.metadata_props"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
