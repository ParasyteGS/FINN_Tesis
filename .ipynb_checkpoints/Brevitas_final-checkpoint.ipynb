{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3362e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8WeightPerTensorFloat\n",
    "\n",
    "\n",
    "class QuantTinyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantTinyCNN, self).__init__()\n",
    "        quantinfo = {\"weight_quant\": Int8WeightPerTensorFloat, \"weight_bit_width\": 8}\n",
    "\n",
    "        self.conv1 = qnn.QuantConv2d(3, 32, kernel_size=3, padding=1, **quantinfo)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=8)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = qnn.QuantConv2d(32, 64, kernel_size=3, padding=1, **quantinfo)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=8)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = qnn.QuantConv2d(64, 128, kernel_size=3, padding=1, **quantinfo)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=8)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = qnn.QuantConv2d(128, 128, kernel_size=3, padding=1, **quantinfo)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=8)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = qnn.QuantLinear(128 * 4 * 4, 100, **quantinfo)\n",
    "        self.relu_fc1 = qnn.QuantReLU(bit_width=8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = qnn.QuantLinear(100, 2, **quantinfo)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = self.pool4(self.relu4(self.conv4(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu_fc1(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30089c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "# Función de cuantización\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.0\n",
    "    qmax = 2.0**num_bits - 1.0\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    return q_x / qmax  # volver a rango [0,1]\n",
    "\n",
    "\n",
    "class CustomQuantDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_mapping = {\"bird\": 0, \"no_bird\": 1}\n",
    "\n",
    "        # Recorre las carpetas \"bird\" y \"no_bird\"\n",
    "        for class_name, label in class_mapping.items():\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(class_folder, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_files[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transform cuantizado\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_quantized = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3),\n",
    "        transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform_quantized = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Crear datasets y loaders\n",
    "train_dataset = CustomQuantDataset(\"dataset_split/train\", transform=transform_quantized)\n",
    "val_dataset = CustomQuantDataset(\"dataset_split/val\", transform=val_transform_quantized)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebaf99f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -1.0  /// Max: 1.0\n",
      "dtype: float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset[10]  # tu dataset cuantizado\n",
    "image = np.array(image)\n",
    "image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "print(\"Min:\", np.min(image), \" /// Max:\", np.max(image))\n",
    "print(\"dtype:\", image.dtype)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c7412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parasyte/.local/lib/python3.10/site-packages/torch/_tensor.py:1645: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1939.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.5371\n",
      "Epoch [2/5], Loss: 0.7645\n",
      "Epoch [3/5], Loss: 0.6286\n",
      "Epoch [4/5], Loss: 0.6441\n",
      "Epoch [5/5], Loss: 0.8551\n",
      "Epoch [6/5], Loss: 0.4669\n",
      "Epoch [7/5], Loss: 0.6494\n",
      "Epoch [8/5], Loss: 0.4802\n",
      "Epoch [9/5], Loss: 0.6517\n",
      "Epoch [10/5], Loss: 0.5791\n",
      "Epoch [11/5], Loss: 0.5534\n",
      "Epoch [12/5], Loss: 0.7044\n",
      "Epoch [13/5], Loss: 0.4370\n",
      "Epoch [14/5], Loss: 0.6414\n",
      "Epoch [15/5], Loss: 0.3431\n",
      "Epoch [16/5], Loss: 0.4064\n",
      "Epoch [17/5], Loss: 0.5785\n",
      "Epoch [18/5], Loss: 0.4013\n",
      "Epoch [19/5], Loss: 0.6130\n",
      "Epoch [20/5], Loss: 0.2950\n",
      "Epoch [21/5], Loss: 0.5132\n",
      "Epoch [22/5], Loss: 0.4394\n",
      "Epoch [23/5], Loss: 0.5125\n",
      "Epoch [24/5], Loss: 0.5761\n",
      "Epoch [25/5], Loss: 0.3880\n",
      "Epoch [26/5], Loss: 0.3845\n",
      "Epoch [27/5], Loss: 0.3967\n",
      "Epoch [28/5], Loss: 0.5529\n",
      "Epoch [29/5], Loss: 0.2524\n",
      "Epoch [30/5], Loss: 0.3471\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "net = QuantTinyCNN().to(device)\n",
    "net.train()\n",
    "num_epochs = 30\n",
    "criterion = nn.CrossEntropyLoss()  # Para clasificación multiclase\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b89d1c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA___slow_conv2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     60\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Brevitas puede requerir float\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(out\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m, in \u001b[0;36mQuantTinyCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/nn/quant_conv.py:197\u001b[0m, in \u001b[0;36mQuantConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[Tensor, QuantTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, QuantTensor]:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/nn/quant_layer.py:158\u001b[0m, in \u001b[0;36mQuantWeightBiasInputOutputLayer.forward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     quant_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m quant_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_quant(output_tensor)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_output(quant_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/nn/quant_conv.py:203\u001b[0m, in \u001b[0;36mQuantConv2d.inner_forward_impl\u001b[0;34m(self, x, quant_weight, quant_bias)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2d_same_zeros_pad_stride(x, quant_weight, quant_bias)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_bias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/quant_tensor/int_quant_tensor.py:49\u001b[0m, in \u001b[0;36mIntQuantTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m INT_QUANT_TENSOR_FN_HANDLER:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mINT_QUANT_TENSOR_FN_HANDLER\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m QUANT_TENSOR_FN_HANDLER:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QUANT_TENSOR_FN_HANDLER[func](\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/quant_tensor/int_torch_handler.py:45\u001b[0m, in \u001b[0;36mconv2d_handler\u001b[0;34m(quant_input, quant_weight, bias, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@implements_int_qt\u001b[39m(F\u001b[38;5;241m.\u001b[39mconv2d)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconv2d_handler\u001b[39m(quant_input, quant_weight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 45\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mquant_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/quant_tensor/int_torch_handler.py:169\u001b[0m, in \u001b[0;36mquant_layer\u001b[0;34m(fn, quant_input, quant_weight, bias, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\n\u001b[1;32m    163\u001b[0m         _unpack_quant_tensor(quant_input),\n\u001b[1;32m    164\u001b[0m         _unpack_quant_tensor(quant_weight),\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quant_input, IntQuantTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quant_weight, IntQuantTensor):\n\u001b[1;32m    177\u001b[0m     output_bit_width \u001b[38;5;241m=\u001b[39m max_acc_bit_width(\n\u001b[1;32m    178\u001b[0m         quant_input\u001b[38;5;241m.\u001b[39mbit_width,\n\u001b[1;32m    179\u001b[0m         quant_weight\u001b[38;5;241m.\u001b[39mbit_width,\n\u001b[1;32m    180\u001b[0m         quant_weight\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but got weight is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA___slow_conv2d_forward)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "# Tu dataset de test personalizado\n",
    "class CustomTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_mapping = {\"bird\": 0, \"no_bird\": 1}\n",
    "        for class_name, label in class_mapping.items():\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(class_folder, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_files[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transform similar al de entrenamiento\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),  # tamaño que tu modelo espera\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(\n",
    "            lambda x: quantize_tensor(x, num_bits=8)\n",
    "        ),  # si tu modelo espera cuantización\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = CustomTestDataset(\"Images_test\", transform=transform_test)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False\n",
    ")  # batch 1 para pocas imágenes\n",
    "\n",
    "# Evaluar\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.float()  # Brevitas puede requerir float\n",
    "        out = net(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy on custom test set:\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c02fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntQuantTensor(value=tensor([[[[-0.0122,  0.2082, -0.0837],\n",
      "          [ 0.2123,  0.0449, -0.0429],\n",
      "          [ 0.1510,  0.1612, -0.1735]],\n",
      "\n",
      "         [[-0.1306, -0.0408, -0.2204],\n",
      "          [ 0.1449, -0.1776, -0.1714],\n",
      "          [-0.0796, -0.1653, -0.1082]],\n",
      "\n",
      "         [[-0.0245, -0.0020,  0.0653],\n",
      "          [ 0.1429,  0.0959, -0.1061],\n",
      "          [ 0.0653,  0.1612,  0.0571]]],\n",
      "\n",
      "\n",
      "        [[[-0.1041,  0.1286, -0.1551],\n",
      "          [ 0.1306, -0.0306,  0.1408],\n",
      "          [ 0.0918, -0.0694, -0.0286]],\n",
      "\n",
      "         [[ 0.0694, -0.1327,  0.0327],\n",
      "          [-0.0939,  0.0796,  0.0694],\n",
      "          [ 0.1510, -0.1163,  0.1367]],\n",
      "\n",
      "         [[-0.0857, -0.1204, -0.0714],\n",
      "          [ 0.2123,  0.0510, -0.1633],\n",
      "          [ 0.0551, -0.0837, -0.0816]]],\n",
      "\n",
      "\n",
      "        [[[-0.0633,  0.1531, -0.0490],\n",
      "          [ 0.0204,  0.1531,  0.0571],\n",
      "          [-0.0000, -0.0755,  0.1061]],\n",
      "\n",
      "         [[ 0.0041, -0.1245,  0.1633],\n",
      "          [-0.1245, -0.1633, -0.1633],\n",
      "          [ 0.0510,  0.1143,  0.0674]],\n",
      "\n",
      "         [[-0.0082, -0.0633, -0.0776],\n",
      "          [-0.0510, -0.1572, -0.0367],\n",
      "          [ 0.1510, -0.0510,  0.1878]]],\n",
      "\n",
      "\n",
      "        [[[-0.1388, -0.0755, -0.1041],\n",
      "          [-0.0531,  0.1041, -0.1306],\n",
      "          [-0.0714,  0.1245, -0.1347]],\n",
      "\n",
      "         [[-0.1633, -0.1674, -0.0429],\n",
      "          [-0.0816,  0.0429,  0.1367],\n",
      "          [ 0.0653, -0.0327, -0.0653]],\n",
      "\n",
      "         [[ 0.2000,  0.0653, -0.0388],\n",
      "          [ 0.0571,  0.1980,  0.1347],\n",
      "          [-0.0225, -0.1000,  0.0939]]],\n",
      "\n",
      "\n",
      "        [[[-0.0612,  0.2286, -0.0633],\n",
      "          [-0.1429,  0.0776,  0.0571],\n",
      "          [-0.1327,  0.2163,  0.1061]],\n",
      "\n",
      "         [[-0.0939, -0.2163,  0.0980],\n",
      "          [ 0.0939, -0.1796, -0.0000],\n",
      "          [-0.0571,  0.0612, -0.1123]],\n",
      "\n",
      "         [[-0.0429, -0.0388,  0.0551],\n",
      "          [-0.0347,  0.0061,  0.0633],\n",
      "          [ 0.0837, -0.0408,  0.1245]]],\n",
      "\n",
      "\n",
      "        [[[-0.0857, -0.0469, -0.0592],\n",
      "          [-0.1959,  0.1163, -0.0408],\n",
      "          [ 0.0551,  0.0857,  0.1612]],\n",
      "\n",
      "         [[-0.0000,  0.0327,  0.1286],\n",
      "          [-0.2143,  0.0980, -0.1327],\n",
      "          [-0.2592, -0.1837, -0.0225]],\n",
      "\n",
      "         [[ 0.1204,  0.1572,  0.0571],\n",
      "          [ 0.0898, -0.1082,  0.0918],\n",
      "          [ 0.1572,  0.0102, -0.1102]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0490,  0.1572, -0.0857],\n",
      "          [ 0.1347, -0.2123, -0.0939],\n",
      "          [ 0.1429,  0.0714,  0.1408]],\n",
      "\n",
      "         [[ 0.1082,  0.1388,  0.1612],\n",
      "          [-0.0796, -0.1123,  0.0347],\n",
      "          [-0.1061, -0.0082,  0.1265]],\n",
      "\n",
      "         [[-0.0143, -0.0245, -0.0939],\n",
      "          [-0.0143,  0.0041,  0.0551],\n",
      "          [-0.0837, -0.2123, -0.0286]]],\n",
      "\n",
      "\n",
      "        [[[-0.1061,  0.0531,  0.0408],\n",
      "          [ 0.0592, -0.0551, -0.0694],\n",
      "          [ 0.1959,  0.1306, -0.0939]],\n",
      "\n",
      "         [[-0.1061,  0.1061, -0.1551],\n",
      "          [-0.0306,  0.1796, -0.1653],\n",
      "          [-0.1367,  0.1204,  0.1510]],\n",
      "\n",
      "         [[-0.0000, -0.1898, -0.0225],\n",
      "          [ 0.1633, -0.1265, -0.0531],\n",
      "          [-0.1000,  0.1980,  0.0531]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0653, -0.0408,  0.2225],\n",
      "          [ 0.1470, -0.1245,  0.1674],\n",
      "          [-0.0918, -0.0612,  0.1796]],\n",
      "\n",
      "         [[-0.2021,  0.0122, -0.0776],\n",
      "          [-0.1592, -0.1306, -0.1510],\n",
      "          [-0.1551, -0.2082,  0.0755]],\n",
      "\n",
      "         [[ 0.1367,  0.1327, -0.0735],\n",
      "          [ 0.0347,  0.0143,  0.0837],\n",
      "          [ 0.1572,  0.1143, -0.1041]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0306, -0.1143, -0.0694],\n",
      "          [ 0.1572,  0.0367, -0.0449],\n",
      "          [ 0.1735,  0.1592,  0.0674]],\n",
      "\n",
      "         [[ 0.0959, -0.0551, -0.1490],\n",
      "          [ 0.0816,  0.0245, -0.0796],\n",
      "          [-0.0959, -0.2163, -0.2143]],\n",
      "\n",
      "         [[ 0.0714, -0.0755,  0.1980],\n",
      "          [-0.1204,  0.0571,  0.1163],\n",
      "          [-0.1061, -0.0020,  0.1510]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0122,  0.1714,  0.1306],\n",
      "          [-0.1367,  0.0980,  0.1388],\n",
      "          [-0.0143, -0.0694, -0.2143]],\n",
      "\n",
      "         [[ 0.0225, -0.1980, -0.1082],\n",
      "          [-0.0204,  0.0408,  0.1286],\n",
      "          [ 0.1286,  0.0327, -0.0327]],\n",
      "\n",
      "         [[ 0.0796, -0.0490,  0.1061],\n",
      "          [-0.0286,  0.1612, -0.0225],\n",
      "          [ 0.0531, -0.0082,  0.1367]]],\n",
      "\n",
      "\n",
      "        [[[-0.0653,  0.0122, -0.1286],\n",
      "          [-0.1510,  0.1959, -0.0653],\n",
      "          [ 0.1674,  0.0306,  0.0531]],\n",
      "\n",
      "         [[-0.0551,  0.0082,  0.1143],\n",
      "          [-0.0694, -0.0510,  0.1388],\n",
      "          [-0.0225, -0.1184,  0.2123]],\n",
      "\n",
      "         [[-0.0490,  0.0755, -0.0898],\n",
      "          [-0.1204,  0.0571, -0.0694],\n",
      "          [ 0.0184,  0.0204, -0.0612]]],\n",
      "\n",
      "\n",
      "        [[[-0.0612, -0.0122,  0.0551],\n",
      "          [-0.0184, -0.1674, -0.0898],\n",
      "          [-0.0490, -0.1857, -0.1184]],\n",
      "\n",
      "         [[ 0.1633, -0.1123, -0.0020],\n",
      "          [ 0.1653,  0.0837, -0.1776],\n",
      "          [-0.1592, -0.0367, -0.1796]],\n",
      "\n",
      "         [[ 0.0429,  0.1878, -0.0163],\n",
      "          [ 0.0633, -0.0143,  0.0143],\n",
      "          [-0.0327,  0.1449, -0.1021]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0469,  0.0878,  0.1510],\n",
      "          [-0.1327, -0.1572, -0.1123],\n",
      "          [-0.1041,  0.1000, -0.0796]],\n",
      "\n",
      "         [[ 0.1245, -0.0367, -0.1674],\n",
      "          [-0.1184,  0.0306, -0.1021],\n",
      "          [ 0.0837, -0.1490,  0.0367]],\n",
      "\n",
      "         [[-0.0918,  0.0571,  0.1817],\n",
      "          [ 0.1980, -0.1163,  0.1286],\n",
      "          [ 0.1041,  0.0408,  0.1061]]],\n",
      "\n",
      "\n",
      "        [[[-0.0286,  0.1959,  0.2000],\n",
      "          [-0.2102, -0.0816,  0.2368],\n",
      "          [-0.1592,  0.0061, -0.0490]],\n",
      "\n",
      "         [[-0.1470,  0.0082,  0.0225],\n",
      "          [-0.0490,  0.1204, -0.0714],\n",
      "          [-0.1000, -0.0184, -0.0265]],\n",
      "\n",
      "         [[-0.1184, -0.0796,  0.0551],\n",
      "          [ 0.0102,  0.0714,  0.0490],\n",
      "          [-0.0592,  0.1000,  0.1939]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2266, -0.0122, -0.1245],\n",
      "          [-0.0143,  0.1000, -0.0571],\n",
      "          [ 0.1939, -0.1633, -0.1347]],\n",
      "\n",
      "         [[ 0.1572,  0.0102, -0.0939],\n",
      "          [ 0.2123, -0.1694, -0.0020],\n",
      "          [ 0.0796, -0.0755, -0.2245]],\n",
      "\n",
      "         [[ 0.0898, -0.0490, -0.0102],\n",
      "          [-0.0429,  0.1367, -0.0898],\n",
      "          [-0.0163, -0.0796,  0.0959]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0286,  0.0796,  0.0653],\n",
      "          [-0.0816, -0.1286,  0.1572],\n",
      "          [ 0.1041, -0.0612,  0.1184]],\n",
      "\n",
      "         [[-0.0388,  0.0449,  0.0469],\n",
      "          [-0.1980, -0.1714, -0.0327],\n",
      "          [-0.0898, -0.0122,  0.0469]],\n",
      "\n",
      "         [[-0.0490,  0.1919,  0.0102],\n",
      "          [-0.0143, -0.0347,  0.1531],\n",
      "          [-0.1653,  0.1857, -0.0020]]],\n",
      "\n",
      "\n",
      "        [[[-0.1163,  0.1510, -0.1429],\n",
      "          [ 0.1041, -0.1286,  0.0367],\n",
      "          [-0.0878, -0.1551,  0.0694]],\n",
      "\n",
      "         [[ 0.1572,  0.1674, -0.0837],\n",
      "          [-0.1735,  0.0204,  0.1123],\n",
      "          [ 0.0776, -0.0531, -0.0122]],\n",
      "\n",
      "         [[ 0.1347, -0.1061,  0.1796],\n",
      "          [-0.1204, -0.1286, -0.1572],\n",
      "          [ 0.1123,  0.0571,  0.1367]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0571,  0.0163,  0.0898],\n",
      "          [ 0.1347,  0.1000, -0.0510],\n",
      "          [ 0.1388,  0.1694,  0.1163]],\n",
      "\n",
      "         [[-0.0714, -0.1531, -0.1714],\n",
      "          [-0.0755,  0.0939, -0.1755],\n",
      "          [-0.0347, -0.0571, -0.0408]],\n",
      "\n",
      "         [[-0.0776,  0.0061, -0.0531],\n",
      "          [ 0.1959,  0.0898, -0.1143],\n",
      "          [-0.0633, -0.0980,  0.0510]]],\n",
      "\n",
      "\n",
      "        [[[-0.0816,  0.0163, -0.1306],\n",
      "          [-0.1408, -0.0490,  0.1184],\n",
      "          [ 0.1021,  0.0531, -0.0694]],\n",
      "\n",
      "         [[-0.1000, -0.0878,  0.0041],\n",
      "          [ 0.1123,  0.1470, -0.0918],\n",
      "          [ 0.0939,  0.1163,  0.0939]],\n",
      "\n",
      "         [[ 0.0857,  0.0449, -0.0306],\n",
      "          [ 0.0633, -0.1837, -0.1878],\n",
      "          [-0.0429,  0.1592, -0.0469]]],\n",
      "\n",
      "\n",
      "        [[[-0.1470,  0.1306,  0.1000],\n",
      "          [-0.1306, -0.1102, -0.1470],\n",
      "          [-0.1755, -0.0082,  0.0571]],\n",
      "\n",
      "         [[-0.1919, -0.0225, -0.0102],\n",
      "          [-0.0061, -0.0122,  0.1082],\n",
      "          [-0.0878, -0.0980, -0.1286]],\n",
      "\n",
      "         [[-0.1021,  0.0857,  0.1694],\n",
      "          [ 0.0776,  0.1735,  0.1837],\n",
      "          [ 0.0633,  0.0653,  0.0939]]],\n",
      "\n",
      "\n",
      "        [[[-0.1939, -0.1388, -0.0225],\n",
      "          [-0.1021,  0.1286,  0.1123],\n",
      "          [ 0.0612,  0.0388, -0.1061]],\n",
      "\n",
      "         [[-0.1286,  0.0000, -0.0571],\n",
      "          [-0.0939,  0.1102,  0.1388],\n",
      "          [-0.0020,  0.0000,  0.1245]],\n",
      "\n",
      "         [[ 0.1061,  0.0674, -0.1470],\n",
      "          [ 0.1184, -0.1000,  0.1225],\n",
      "          [-0.0163,  0.1163,  0.0980]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0857,  0.0020,  0.1327],\n",
      "          [ 0.1245,  0.1449, -0.0286],\n",
      "          [-0.0490, -0.0959, -0.0776]],\n",
      "\n",
      "         [[ 0.2061,  0.0816,  0.1082],\n",
      "          [ 0.1714, -0.0122, -0.0980],\n",
      "          [-0.1796, -0.2061, -0.0510]],\n",
      "\n",
      "         [[ 0.0429, -0.0449, -0.1572],\n",
      "          [ 0.0306, -0.1694, -0.0816],\n",
      "          [ 0.0612,  0.1572, -0.0225]]],\n",
      "\n",
      "\n",
      "        [[[-0.1653,  0.0612,  0.1184],\n",
      "          [-0.0490,  0.0878, -0.1143],\n",
      "          [-0.0531,  0.1796, -0.1061]],\n",
      "\n",
      "         [[ 0.0347,  0.0347, -0.0429],\n",
      "          [-0.0306,  0.2041,  0.0714],\n",
      "          [-0.2163, -0.1939,  0.0714]],\n",
      "\n",
      "         [[-0.0490, -0.0265,  0.0551],\n",
      "          [ 0.0000,  0.0510,  0.1429],\n",
      "          [-0.1735, -0.0184,  0.2225]]],\n",
      "\n",
      "\n",
      "        [[[-0.1612,  0.2041,  0.1735],\n",
      "          [-0.0510,  0.0531, -0.1245],\n",
      "          [ 0.0980, -0.0367,  0.1735]],\n",
      "\n",
      "         [[-0.1857, -0.0102, -0.1633],\n",
      "          [ 0.0755,  0.1061,  0.1102],\n",
      "          [-0.0265,  0.0000, -0.1327]],\n",
      "\n",
      "         [[-0.0857,  0.0653, -0.0163],\n",
      "          [ 0.0449, -0.1306, -0.1000],\n",
      "          [ 0.0082,  0.1939, -0.0674]]],\n",
      "\n",
      "\n",
      "        [[[-0.0531, -0.0143, -0.0061],\n",
      "          [-0.0592, -0.0204, -0.0286],\n",
      "          [-0.2041,  0.1041, -0.0878]],\n",
      "\n",
      "         [[-0.0918,  0.1306, -0.0020],\n",
      "          [-0.1755, -0.0082, -0.1939],\n",
      "          [ 0.0469,  0.0796,  0.1367]],\n",
      "\n",
      "         [[ 0.1837,  0.0469,  0.0082],\n",
      "          [ 0.0245, -0.0449,  0.1490],\n",
      "          [ 0.1694,  0.0388,  0.0857]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0306,  0.0020, -0.1531],\n",
      "          [-0.1817, -0.0837, -0.1939],\n",
      "          [-0.0327,  0.1347, -0.0592]],\n",
      "\n",
      "         [[ 0.0653,  0.0857,  0.1245],\n",
      "          [ 0.1510, -0.0939, -0.0347],\n",
      "          [ 0.0469, -0.0918,  0.0510]],\n",
      "\n",
      "         [[ 0.0429,  0.0490,  0.1408],\n",
      "          [ 0.1551,  0.1021, -0.0306],\n",
      "          [ 0.0388,  0.1796,  0.1163]]],\n",
      "\n",
      "\n",
      "        [[[-0.1470, -0.0327,  0.0367],\n",
      "          [-0.1163, -0.1184, -0.1470],\n",
      "          [-0.0816, -0.0796,  0.1653]],\n",
      "\n",
      "         [[-0.1021, -0.0939, -0.0980],\n",
      "          [ 0.0122,  0.0449, -0.0612],\n",
      "          [ 0.1245, -0.1184, -0.0633]],\n",
      "\n",
      "         [[ 0.1612, -0.1286,  0.1878],\n",
      "          [-0.0102,  0.1694, -0.0776],\n",
      "          [ 0.0959, -0.0837,  0.0612]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1367, -0.1143, -0.1041],\n",
      "          [ 0.0878, -0.0163, -0.0388],\n",
      "          [ 0.0878, -0.0959,  0.0918]],\n",
      "\n",
      "         [[ 0.0571,  0.1225,  0.1000],\n",
      "          [ 0.1347,  0.1021,  0.0143],\n",
      "          [ 0.0612, -0.0714, -0.1204]],\n",
      "\n",
      "         [[ 0.1306, -0.1102,  0.0531],\n",
      "          [ 0.1551,  0.0286,  0.0429],\n",
      "          [-0.0694,  0.0633,  0.1633]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1163, -0.1123, -0.0592],\n",
      "          [ 0.0878, -0.1184, -0.1714],\n",
      "          [-0.1510, -0.1612,  0.0245]],\n",
      "\n",
      "         [[ 0.1531,  0.0102,  0.1592],\n",
      "          [ 0.0837, -0.0306, -0.0959],\n",
      "          [ 0.1000,  0.1572,  0.1633]],\n",
      "\n",
      "         [[-0.0980, -0.0490,  0.1470],\n",
      "          [-0.0816, -0.0837,  0.0265],\n",
      "          [ 0.1653,  0.0306,  0.0286]]],\n",
      "\n",
      "\n",
      "        [[[-0.0694,  0.1163, -0.1245],\n",
      "          [ 0.1551,  0.0469,  0.0408],\n",
      "          [-0.0000,  0.0592,  0.0816]],\n",
      "\n",
      "         [[-0.0388,  0.1429, -0.1470],\n",
      "          [ 0.1245,  0.1694,  0.1837],\n",
      "          [-0.0776, -0.1470, -0.0714]],\n",
      "\n",
      "         [[-0.1327, -0.0510,  0.0184],\n",
      "          [-0.1694,  0.1776, -0.1837],\n",
      "          [-0.0735,  0.1082, -0.0184]]],\n",
      "\n",
      "\n",
      "        [[[-0.0612, -0.1306, -0.1714],\n",
      "          [-0.1143,  0.0959, -0.0184],\n",
      "          [-0.0204, -0.0653,  0.1714]],\n",
      "\n",
      "         [[-0.0449, -0.0245, -0.0653],\n",
      "          [ 0.0225,  0.0184,  0.1449],\n",
      "          [-0.1612, -0.1163, -0.0020]],\n",
      "\n",
      "         [[ 0.0796,  0.0959, -0.0327],\n",
      "          [ 0.1367,  0.0286, -0.0571],\n",
      "          [ 0.1898,  0.1327,  0.2000]]]], grad_fn=<MulBackward0>), scale=0.0020410132128745317, zero_point=0.0, bit_width=8.0, signed_t=True, training_t=False)\n",
      "tensor([[[[  -6,  102,  -41],\n",
      "          [ 104,   22,  -21],\n",
      "          [  74,   79,  -85]],\n",
      "\n",
      "         [[ -64,  -20, -108],\n",
      "          [  71,  -87,  -84],\n",
      "          [ -39,  -81,  -53]],\n",
      "\n",
      "         [[ -12,   -1,   32],\n",
      "          [  70,   47,  -52],\n",
      "          [  32,   79,   28]]],\n",
      "\n",
      "\n",
      "        [[[ -51,   63,  -76],\n",
      "          [  64,  -15,   69],\n",
      "          [  45,  -34,  -14]],\n",
      "\n",
      "         [[  34,  -65,   16],\n",
      "          [ -46,   39,   34],\n",
      "          [  74,  -57,   67]],\n",
      "\n",
      "         [[ -42,  -59,  -35],\n",
      "          [ 104,   25,  -80],\n",
      "          [  27,  -41,  -40]]],\n",
      "\n",
      "\n",
      "        [[[ -31,   75,  -24],\n",
      "          [  10,   75,   28],\n",
      "          [   0,  -37,   52]],\n",
      "\n",
      "         [[   2,  -61,   80],\n",
      "          [ -61,  -80,  -80],\n",
      "          [  25,   56,   33]],\n",
      "\n",
      "         [[  -4,  -31,  -38],\n",
      "          [ -25,  -77,  -18],\n",
      "          [  74,  -25,   92]]],\n",
      "\n",
      "\n",
      "        [[[ -68,  -37,  -51],\n",
      "          [ -26,   51,  -64],\n",
      "          [ -35,   61,  -66]],\n",
      "\n",
      "         [[ -80,  -82,  -21],\n",
      "          [ -40,   21,   67],\n",
      "          [  32,  -16,  -32]],\n",
      "\n",
      "         [[  98,   32,  -19],\n",
      "          [  28,   97,   66],\n",
      "          [ -11,  -49,   46]]],\n",
      "\n",
      "\n",
      "        [[[ -30,  112,  -31],\n",
      "          [ -70,   38,   28],\n",
      "          [ -65,  106,   52]],\n",
      "\n",
      "         [[ -46, -106,   48],\n",
      "          [  46,  -88,    0],\n",
      "          [ -28,   30,  -55]],\n",
      "\n",
      "         [[ -21,  -19,   27],\n",
      "          [ -17,    3,   31],\n",
      "          [  41,  -20,   61]]],\n",
      "\n",
      "\n",
      "        [[[ -42,  -23,  -29],\n",
      "          [ -96,   57,  -20],\n",
      "          [  27,   42,   79]],\n",
      "\n",
      "         [[   0,   16,   63],\n",
      "          [-105,   48,  -65],\n",
      "          [-127,  -90,  -11]],\n",
      "\n",
      "         [[  59,   77,   28],\n",
      "          [  44,  -53,   45],\n",
      "          [  77,    5,  -54]]],\n",
      "\n",
      "\n",
      "        [[[  24,   77,  -42],\n",
      "          [  66, -104,  -46],\n",
      "          [  70,   35,   69]],\n",
      "\n",
      "         [[  53,   68,   79],\n",
      "          [ -39,  -55,   17],\n",
      "          [ -52,   -4,   62]],\n",
      "\n",
      "         [[  -7,  -12,  -46],\n",
      "          [  -7,    2,   27],\n",
      "          [ -41, -104,  -14]]],\n",
      "\n",
      "\n",
      "        [[[ -52,   26,   20],\n",
      "          [  29,  -27,  -34],\n",
      "          [  96,   64,  -46]],\n",
      "\n",
      "         [[ -52,   52,  -76],\n",
      "          [ -15,   88,  -81],\n",
      "          [ -67,   59,   74]],\n",
      "\n",
      "         [[   0,  -93,  -11],\n",
      "          [  80,  -62,  -26],\n",
      "          [ -49,   97,   26]]],\n",
      "\n",
      "\n",
      "        [[[  32,  -20,  109],\n",
      "          [  72,  -61,   82],\n",
      "          [ -45,  -30,   88]],\n",
      "\n",
      "         [[ -99,    6,  -38],\n",
      "          [ -78,  -64,  -74],\n",
      "          [ -76, -102,   37]],\n",
      "\n",
      "         [[  67,   65,  -36],\n",
      "          [  17,    7,   41],\n",
      "          [  77,   56,  -51]]],\n",
      "\n",
      "\n",
      "        [[[  15,  -56,  -34],\n",
      "          [  77,   18,  -22],\n",
      "          [  85,   78,   33]],\n",
      "\n",
      "         [[  47,  -27,  -73],\n",
      "          [  40,   12,  -39],\n",
      "          [ -47, -106, -105]],\n",
      "\n",
      "         [[  35,  -37,   97],\n",
      "          [ -59,   28,   57],\n",
      "          [ -52,   -1,   74]]],\n",
      "\n",
      "\n",
      "        [[[   6,   84,   64],\n",
      "          [ -67,   48,   68],\n",
      "          [  -7,  -34, -105]],\n",
      "\n",
      "         [[  11,  -97,  -53],\n",
      "          [ -10,   20,   63],\n",
      "          [  63,   16,  -16]],\n",
      "\n",
      "         [[  39,  -24,   52],\n",
      "          [ -14,   79,  -11],\n",
      "          [  26,   -4,   67]]],\n",
      "\n",
      "\n",
      "        [[[ -32,    6,  -63],\n",
      "          [ -74,   96,  -32],\n",
      "          [  82,   15,   26]],\n",
      "\n",
      "         [[ -27,    4,   56],\n",
      "          [ -34,  -25,   68],\n",
      "          [ -11,  -58,  104]],\n",
      "\n",
      "         [[ -24,   37,  -44],\n",
      "          [ -59,   28,  -34],\n",
      "          [   9,   10,  -30]]],\n",
      "\n",
      "\n",
      "        [[[ -30,   -6,   27],\n",
      "          [  -9,  -82,  -44],\n",
      "          [ -24,  -91,  -58]],\n",
      "\n",
      "         [[  80,  -55,   -1],\n",
      "          [  81,   41,  -87],\n",
      "          [ -78,  -18,  -88]],\n",
      "\n",
      "         [[  21,   92,   -8],\n",
      "          [  31,   -7,    7],\n",
      "          [ -16,   71,  -50]]],\n",
      "\n",
      "\n",
      "        [[[  23,   43,   74],\n",
      "          [ -65,  -77,  -55],\n",
      "          [ -51,   49,  -39]],\n",
      "\n",
      "         [[  61,  -18,  -82],\n",
      "          [ -58,   15,  -50],\n",
      "          [  41,  -73,   18]],\n",
      "\n",
      "         [[ -45,   28,   89],\n",
      "          [  97,  -57,   63],\n",
      "          [  51,   20,   52]]],\n",
      "\n",
      "\n",
      "        [[[ -14,   96,   98],\n",
      "          [-103,  -40,  116],\n",
      "          [ -78,    3,  -24]],\n",
      "\n",
      "         [[ -72,    4,   11],\n",
      "          [ -24,   59,  -35],\n",
      "          [ -49,   -9,  -13]],\n",
      "\n",
      "         [[ -58,  -39,   27],\n",
      "          [   5,   35,   24],\n",
      "          [ -29,   49,   95]]],\n",
      "\n",
      "\n",
      "        [[[ 111,   -6,  -61],\n",
      "          [  -7,   49,  -28],\n",
      "          [  95,  -80,  -66]],\n",
      "\n",
      "         [[  77,    5,  -46],\n",
      "          [ 104,  -83,   -1],\n",
      "          [  39,  -37, -110]],\n",
      "\n",
      "         [[  44,  -24,   -5],\n",
      "          [ -21,   67,  -44],\n",
      "          [  -8,  -39,   47]]],\n",
      "\n",
      "\n",
      "        [[[  14,   39,   32],\n",
      "          [ -40,  -63,   77],\n",
      "          [  51,  -30,   58]],\n",
      "\n",
      "         [[ -19,   22,   23],\n",
      "          [ -97,  -84,  -16],\n",
      "          [ -44,   -6,   23]],\n",
      "\n",
      "         [[ -24,   94,    5],\n",
      "          [  -7,  -17,   75],\n",
      "          [ -81,   91,   -1]]],\n",
      "\n",
      "\n",
      "        [[[ -57,   74,  -70],\n",
      "          [  51,  -63,   18],\n",
      "          [ -43,  -76,   34]],\n",
      "\n",
      "         [[  77,   82,  -41],\n",
      "          [ -85,   10,   55],\n",
      "          [  38,  -26,   -6]],\n",
      "\n",
      "         [[  66,  -52,   88],\n",
      "          [ -59,  -63,  -77],\n",
      "          [  55,   28,   67]]],\n",
      "\n",
      "\n",
      "        [[[  28,    8,   44],\n",
      "          [  66,   49,  -25],\n",
      "          [  68,   83,   57]],\n",
      "\n",
      "         [[ -35,  -75,  -84],\n",
      "          [ -37,   46,  -86],\n",
      "          [ -17,  -28,  -20]],\n",
      "\n",
      "         [[ -38,    3,  -26],\n",
      "          [  96,   44,  -56],\n",
      "          [ -31,  -48,   25]]],\n",
      "\n",
      "\n",
      "        [[[ -40,    8,  -64],\n",
      "          [ -69,  -24,   58],\n",
      "          [  50,   26,  -34]],\n",
      "\n",
      "         [[ -49,  -43,    2],\n",
      "          [  55,   72,  -45],\n",
      "          [  46,   57,   46]],\n",
      "\n",
      "         [[  42,   22,  -15],\n",
      "          [  31,  -90,  -92],\n",
      "          [ -21,   78,  -23]]],\n",
      "\n",
      "\n",
      "        [[[ -72,   64,   49],\n",
      "          [ -64,  -54,  -72],\n",
      "          [ -86,   -4,   28]],\n",
      "\n",
      "         [[ -94,  -11,   -5],\n",
      "          [  -3,   -6,   53],\n",
      "          [ -43,  -48,  -63]],\n",
      "\n",
      "         [[ -50,   42,   83],\n",
      "          [  38,   85,   90],\n",
      "          [  31,   32,   46]]],\n",
      "\n",
      "\n",
      "        [[[ -95,  -68,  -11],\n",
      "          [ -50,   63,   55],\n",
      "          [  30,   19,  -52]],\n",
      "\n",
      "         [[ -63,    0,  -28],\n",
      "          [ -46,   54,   68],\n",
      "          [  -1,    0,   61]],\n",
      "\n",
      "         [[  52,   33,  -72],\n",
      "          [  58,  -49,   60],\n",
      "          [  -8,   57,   48]]],\n",
      "\n",
      "\n",
      "        [[[  42,    1,   65],\n",
      "          [  61,   71,  -14],\n",
      "          [ -24,  -47,  -38]],\n",
      "\n",
      "         [[ 101,   40,   53],\n",
      "          [  84,   -6,  -48],\n",
      "          [ -88, -101,  -25]],\n",
      "\n",
      "         [[  21,  -22,  -77],\n",
      "          [  15,  -83,  -40],\n",
      "          [  30,   77,  -11]]],\n",
      "\n",
      "\n",
      "        [[[ -81,   30,   58],\n",
      "          [ -24,   43,  -56],\n",
      "          [ -26,   88,  -52]],\n",
      "\n",
      "         [[  17,   17,  -21],\n",
      "          [ -15,  100,   35],\n",
      "          [-106,  -95,   35]],\n",
      "\n",
      "         [[ -24,  -13,   27],\n",
      "          [   0,   25,   70],\n",
      "          [ -85,   -9,  109]]],\n",
      "\n",
      "\n",
      "        [[[ -79,  100,   85],\n",
      "          [ -25,   26,  -61],\n",
      "          [  48,  -18,   85]],\n",
      "\n",
      "         [[ -91,   -5,  -80],\n",
      "          [  37,   52,   54],\n",
      "          [ -13,    0,  -65]],\n",
      "\n",
      "         [[ -42,   32,   -8],\n",
      "          [  22,  -64,  -49],\n",
      "          [   4,   95,  -33]]],\n",
      "\n",
      "\n",
      "        [[[ -26,   -7,   -3],\n",
      "          [ -29,  -10,  -14],\n",
      "          [-100,   51,  -43]],\n",
      "\n",
      "         [[ -45,   64,   -1],\n",
      "          [ -86,   -4,  -95],\n",
      "          [  23,   39,   67]],\n",
      "\n",
      "         [[  90,   23,    4],\n",
      "          [  12,  -22,   73],\n",
      "          [  83,   19,   42]]],\n",
      "\n",
      "\n",
      "        [[[  15,    1,  -75],\n",
      "          [ -89,  -41,  -95],\n",
      "          [ -16,   66,  -29]],\n",
      "\n",
      "         [[  32,   42,   61],\n",
      "          [  74,  -46,  -17],\n",
      "          [  23,  -45,   25]],\n",
      "\n",
      "         [[  21,   24,   69],\n",
      "          [  76,   50,  -15],\n",
      "          [  19,   88,   57]]],\n",
      "\n",
      "\n",
      "        [[[ -72,  -16,   18],\n",
      "          [ -57,  -58,  -72],\n",
      "          [ -40,  -39,   81]],\n",
      "\n",
      "         [[ -50,  -46,  -48],\n",
      "          [   6,   22,  -30],\n",
      "          [  61,  -58,  -31]],\n",
      "\n",
      "         [[  79,  -63,   92],\n",
      "          [  -5,   83,  -38],\n",
      "          [  47,  -41,   30]]],\n",
      "\n",
      "\n",
      "        [[[  67,  -56,  -51],\n",
      "          [  43,   -8,  -19],\n",
      "          [  43,  -47,   45]],\n",
      "\n",
      "         [[  28,   60,   49],\n",
      "          [  66,   50,    7],\n",
      "          [  30,  -35,  -59]],\n",
      "\n",
      "         [[  64,  -54,   26],\n",
      "          [  76,   14,   21],\n",
      "          [ -34,   31,   80]]],\n",
      "\n",
      "\n",
      "        [[[  57,  -55,  -29],\n",
      "          [  43,  -58,  -84],\n",
      "          [ -74,  -79,   12]],\n",
      "\n",
      "         [[  75,    5,   78],\n",
      "          [  41,  -15,  -47],\n",
      "          [  49,   77,   80]],\n",
      "\n",
      "         [[ -48,  -24,   72],\n",
      "          [ -40,  -41,   13],\n",
      "          [  81,   15,   14]]],\n",
      "\n",
      "\n",
      "        [[[ -34,   57,  -61],\n",
      "          [  76,   23,   20],\n",
      "          [   0,   29,   40]],\n",
      "\n",
      "         [[ -19,   70,  -72],\n",
      "          [  61,   83,   90],\n",
      "          [ -38,  -72,  -35]],\n",
      "\n",
      "         [[ -65,  -25,    9],\n",
      "          [ -83,   87,  -90],\n",
      "          [ -36,   53,   -9]]],\n",
      "\n",
      "\n",
      "        [[[ -30,  -64,  -84],\n",
      "          [ -56,   47,   -9],\n",
      "          [ -10,  -32,   84]],\n",
      "\n",
      "         [[ -22,  -12,  -32],\n",
      "          [  11,    9,   71],\n",
      "          [ -79,  -57,   -1]],\n",
      "\n",
      "         [[  39,   47,  -16],\n",
      "          [  67,   14,  -28],\n",
      "          [  93,   65,   98]]]], dtype=torch.int8)\n",
      "torch.int8\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.quant_weight())  # pesos cuantizados\n",
    "print(net.conv1.quant_weight().int()) \n",
    "print(net.conv1.quant_weight().int().dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773a29f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "# -------------------------\n",
    "# Configuración\n",
    "# -------------------------\n",
    "root_dir = \"./\"  # Carpeta donde se guardarán los ONNX\n",
    "dataset_path = \"./Images_test\"  # Carpeta con tus imágenes\n",
    "image_size = (64, 64)  # Cambia según tu dataset\n",
    "num_bits = 8  # Cuantización\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Función de cuantización asimétrica\n",
    "# -------------------------\n",
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    min_val = 0\n",
    "    max_val = 2**num_bits - 1\n",
    "\n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / max_val\n",
    "    zero_point = np.clip((-beta / scale), 0, max_val).round().astype(np.int8)\n",
    "\n",
    "    quantized_arr = np.clip(\n",
    "        np.round(arr / scale + zero_point), min_val, max_val\n",
    "    ).astype(np.float32)\n",
    "    return quantized_arr\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Cargar una imagen y preprocesar\n",
    "# -------------------------\n",
    "def load_and_quantize_image(file_path):\n",
    "    img = Image.open(file_path).convert(\"RGB\")\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img).astype(np.float32) / 255.0  # normalización a 0-1\n",
    "    img_arr = img_arr.transpose(2, 0, 1)  # (H,W,C) -> (C,H,W)\n",
    "    img_q = asymmetric_quantize(img_arr, num_bits=num_bits)\n",
    "    return img_q\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Crear tensor de entrada\n",
    "# -------------------------\n",
    "# Tomamos una imagen de ejemplo (FINN necesita un batch)\n",
    "example_image = load_and_quantize_image(\n",
    "    os.path.join(dataset_path, os.listdir(dataset_path)[0])\n",
    ")\n",
    "input_t = torch.from_numpy(example_image).unsqueeze(0)  # batch dimension\n",
    "\n",
    "# -------------------------\n",
    "# Exportar modelo Brevitas a QONNX\n",
    "# -------------------------\n",
    "# Suponiendo que tu modelo ya está definido y cuantizado\n",
    "# brevitas_model = tu_modelo_brevitas\n",
    "filename_onnx = os.path.join(root_dir, \"part1.onnx\")\n",
    "filename_clean = os.path.join(root_dir, \"part1_clean.onnx\")\n",
    "\n",
    "export_qonnx(net, export_path=filename_onnx, input_t=input_t)\n",
    "\n",
    "# -------------------------\n",
    "# Limpiar ONNX para FINN\n",
    "# -------------------------\n",
    "qonnx_cleanup(filename_onnx, out_file=filename_clean)\n",
    "\n",
    "# -------------------------\n",
    "# Convertir a FINN\n",
    "# -------------------------\n",
    "model_finn = ModelWrapper(filename_clean)\n",
    "model_finn = model_finn.transform(ConvertQONNXtoFINN())\n",
    "model_finn.save(os.path.join(root_dir, \"ready_finn.onnx\"))\n",
    "\n",
    "print(\"Modelo exportado a FINN en: \" + os.path.join(root_dir, \"ready_finn.onnx\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
