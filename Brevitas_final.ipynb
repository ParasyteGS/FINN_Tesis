{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d3362e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8WeightPerTensorFloat, Int8ActPerTensorFloat\n",
    "\n",
    "\n",
    "class QuantTinyCNN_FINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantTinyCNN_FINN, self).__init__()\n",
    "        quantinfo = {\"weight_quant\": Int8WeightPerTensorFloat, \"weight_bit_width\": 8}\n",
    "\n",
    "        # ⚠️ Eliminamos conv+pool (FINN no lo soporta bien)\n",
    "        # Usamos solo capas fully-connected cuantizadas\n",
    "\n",
    "        self.fc1 = qnn.QuantLinear(\n",
    "            in_features=3 * 64 *64,  # suponiendo imágenes de 32x32 RGB\n",
    "            out_features=512,\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=8, act_quant=Int8ActPerTensorFloat)\n",
    "\n",
    "        self.fc2 = qnn.QuantLinear(\n",
    "            in_features=512,\n",
    "            out_features=256,\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=8, act_quant=Int8ActPerTensorFloat)\n",
    "\n",
    "        self.fc3 = qnn.QuantLinear(\n",
    "            in_features=256,\n",
    "            out_features=100,\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=8, act_quant=Int8ActPerTensorFloat)\n",
    "\n",
    "        self.fc4 = qnn.QuantLinear(\n",
    "            in_features=100,\n",
    "            out_features=2,  # salida para 2 clases\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplanar entrada (ya no hay conv)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a30089c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "# Función de cuantización\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.0\n",
    "    qmax = 2.0**num_bits - 1.0\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    return q_x / qmax  # volver a rango [0,1]\n",
    "\n",
    "\n",
    "class CustomQuantDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_mapping = {\"bird\": 0, \"no_bird\": 1}\n",
    "\n",
    "        # Recorre las carpetas \"bird\" y \"no_bird\"\n",
    "        for class_name, label in class_mapping.items():\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(class_folder, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_files[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transform cuantizado\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_quantized = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3),\n",
    "        transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform_quantized = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Crear datasets y loaders\n",
    "train_dataset = CustomQuantDataset(\"dataset_split/train\", transform=transform_quantized)\n",
    "val_dataset = CustomQuantDataset(\"dataset_split/val\", transform=val_transform_quantized)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebaf99f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -0.6392157  /// Max: 1.0\n",
      "dtype: float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image, label = train_dataset[10]  # tu dataset cuantizado\n",
    "image = np.array(image)\n",
    "image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "print(\"Min:\", np.min(image), \" /// Max:\", np.max(image))\n",
    "print(\"dtype:\", image.dtype)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "252c7412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6257\n",
      "Epoch [2/5], Loss: 0.8559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 49\u001b[0m, in \u001b[0;36mQuantTinyCNN_FINN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Aplanar entrada (ya no hay conv)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/nn/quant_linear.py:66\u001b[0m, in \u001b[0;36mQuantLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[Tensor, QuantTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, QuantTensor]:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/nn/quant_layer.py:311\u001b[0m, in \u001b[0;36mQuantWeightBiasInputOutputLayer.forward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    310\u001b[0m quant_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_quant(inp)\n\u001b[0;32m--> 311\u001b[0m quant_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_quant_tensor \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_bias_quant_enabled \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    315\u001b[0m      (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_quant\u001b[38;5;241m.\u001b[39mrequires_input_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_quant\u001b[38;5;241m.\u001b[39mrequires_input_bit_width))):\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m quant_input\u001b[38;5;241m.\u001b[39mbit_width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m quant_weight\u001b[38;5;241m.\u001b[39mbit_width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/nn/mixin/parameter.py:103\u001b[0m, in \u001b[0;36mQuantWeightMixin.quant_weight\u001b[0;34m(self, quant_input, subtensor_slice_list)\u001b[0m\n\u001b[1;32m    100\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_quant(\n\u001b[1;32m    101\u001b[0m         weights_to_quantize[weight_slice_tuple], input_bit_width, input_is_signed)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_to_quantize\u001b[49m\u001b[43m[\u001b[49m\u001b[43mweight_slice_tuple\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subtensor_slice_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Restore the quantizer behaviour to full tensor quantization\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# The modules to slice should have been cached already at this point\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_sub_tensor_slice_list_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing cache of modules to slice.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/proxy/parameter_quant.py:100\u001b[0m, in \u001b[0;36mWeightQuantProxyFromInjector.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_quant_enabled:\n\u001b[1;32m     99\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_handler \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_quant\n\u001b[0;32m--> 100\u001b[0m     out, scale, zero_point, bit_width \u001b[38;5;241m=\u001b[39m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QuantTensor(out, scale, zero_point, bit_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_signed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# quantization disabled\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/core/quant/int.py:156\u001b[0m, in \u001b[0;36mRescalingIntQuant.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m scale \u001b[38;5;241m=\u001b[39m threshold \u001b[38;5;241m/\u001b[39m int_threshold\n\u001b[1;32m    155\u001b[0m zero_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_point_impl(x, scale, bit_width)\n\u001b[0;32m--> 156\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, scale, zero_point, bit_width\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/core/quant/int_base.py:84\u001b[0m, in \u001b[0;36mIntQuant.forward\u001b[0;34m(self, scale, zero_point, bit_width, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, scale: Tensor, zero_point: Tensor, bit_width: Tensor, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 84\u001b[0m     y_int \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     y \u001b[38;5;241m=\u001b[39m y_int \u001b[38;5;241m-\u001b[39m zero_point\n\u001b[1;32m     86\u001b[0m     y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m*\u001b[39m scale\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/core/quant/int_base.py:71\u001b[0m, in \u001b[0;36mIntQuant.to_int\u001b[0;34m(self, scale, zero_point, bit_width, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m max_int_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_int(bit_width)\n\u001b[1;32m     70\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloat_to_int_impl(y)\n\u001b[0;32m---> 71\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_clamp_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_int_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_int_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/core/function_wrapper/ops_ste.py:105\u001b[0m, in \u001b[0;36mTensorClampSte.forward\u001b[0;34m(self, x, min_val, max_val)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, min_val: torch\u001b[38;5;241m.\u001b[39mTensor, max_val: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_clamp_ste\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/function/ops_ste.py:142\u001b[0m, in \u001b[0;36mtensor_clamp_ste\u001b[0;34m(x, min_val, max_val)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor_clamp(x, min_val, max_val)\n\u001b[0;32m--> 142\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfn_prefix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd_ste_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_clamp_ste_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/ops/autograd_ste_ops.py:117\u001b[0m, in \u001b[0;36mTensorClampSteFn.forward\u001b[0;34m(ctx, x, min_val, max_val)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(ctx, x: Tensor, min_val: Tensor, max_val: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_clamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/function/ops.py:96\u001b[0m, in \u001b[0;36mtensor_clamp\u001b[0;34m(x, min_val, max_val)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtensor_clamp\u001b[39m(x: Tensor, min_val: Tensor, max_val: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    Generalized clamp function with support for tensors as clamping values.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        tensor([1.0000, 0.0000, 0.1000])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(out \u001b[38;5;241m<\u001b[39m min_val, min_val\u001b[38;5;241m.\u001b[39mtype_as(out), out)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "net = QuantTinyCNN_FINN().to(device)\n",
    "net.train()\n",
    "num_epochs = 30\n",
    "criterion = nn.CrossEntropyLoss()  # Para clasificación multiclase\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b89d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on custom test set: 73.91304347826087 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "# Tu dataset de test personalizado\n",
    "class CustomTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_mapping = {\"bird\": 0, \"no_bird\": 1}\n",
    "        for class_name, label in class_mapping.items():\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(class_folder, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_files[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transform similar al de entrenamiento\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),  # tamaño que tu modelo espera\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(\n",
    "            lambda x: quantize_tensor(x, num_bits=8)\n",
    "        ),  # si tu modelo espera cuantización\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = CustomTestDataset(\"Images_test\", transform=transform_test)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False\n",
    ")  # batch 1 para pocas imágenes\n",
    "\n",
    "# Evaluar\n",
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.float()  # Brevitas puede requerir float\n",
    "        out = net(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy on custom test set:\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c02fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantTensor(value=tensor([[ 0.0364,  0.0390,  0.0264,  ..., -0.0038,  0.0038,  0.0050],\n",
      "        [ 0.0025,  0.0013,  0.0013,  ...,  0.0302,  0.0138,  0.0276],\n",
      "        [ 0.0101,  0.0088, -0.0013,  ...,  0.0163,  0.0201,  0.0251],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0138,  0.0088,  ...,  0.0013,  0.0013,  0.0038],\n",
      "        [ 0.0214,  0.0264,  0.0390,  ...,  0.0201,  0.0201,  0.0188],\n",
      "        [ 0.0101,  0.0050, -0.0088,  ...,  0.0239,  0.0352,  0.0402]],\n",
      "       grad_fn=<MulBackward0>), scale=tensor(0.0013, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(False))\n",
      "tensor([[29, 31, 21,  ..., -3,  3,  4],\n",
      "        [ 2,  1,  1,  ..., 24, 11, 22],\n",
      "        [ 8,  7, -1,  ..., 13, 16, 20],\n",
      "        ...,\n",
      "        [ 1, 11,  7,  ...,  1,  1,  3],\n",
      "        [17, 21, 31,  ..., 16, 16, 15],\n",
      "        [ 8,  4, -7,  ..., 19, 28, 32]], dtype=torch.int8)\n",
      "torch.int8\n"
     ]
    }
   ],
   "source": [
    "print(net.fc1.quant_weight())  # pesos cuantizados\n",
    "print(net.fc1.quant_weight().int()) \n",
    "print(net.fc1.quant_weight().int().dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "773a29f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m filename_onnx \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart1.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m filename_clean \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart1_clean.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m \u001b[43mexport_qonnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename_onnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Limpiar ONNX para FINN\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m     86\u001b[0m qonnx_cleanup(filename_onnx, out_file\u001b[38;5;241m=\u001b[39mfilename_clean)\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/export/__init__.py:21\u001b[0m, in \u001b[0;36mexport_qonnx\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(QONNXManager\u001b[38;5;241m.\u001b[39mexport)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexport_qonnx\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQONNXManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/export/onnx/manager.py:162\u001b[0m, in \u001b[0;36mONNXBaseManager.export\u001b[0;34m(cls, module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39monnx_export_kwargs):\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_onnx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_warnings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43monnx_export_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/export/onnx/manager.py:113\u001b[0m, in \u001b[0;36mONNXBaseManager.export_onnx\u001b[0;34m(cls, module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_inp_out(module, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_inp_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Dequantize QuantTensor, if any and enabled\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, QuantTensor):\n",
      "File \u001b[0;32m/home/parasyte/finn/deps/brevitas/src/brevitas/export/manager.py:260\u001b[0m, in \u001b[0;36mBaseManager._cache_inp_out\u001b[0;34m(cls, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mgr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_patches():\n\u001b[1;32m    259\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(mgr)\n\u001b[0;32m--> 260\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Restore previous caching properties\u001b[39;00m\n\u001b[1;32m    262\u001b[0m module\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m m: _restore_quant_metadata_caching_mode(m))\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mQuantTinyCNN_FINN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Aplanar entrada (ya no hay conv)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "\n",
    "user_name = \"parasyte\" # REPLACE THIS WITH YOUR HOST MACHINE USER NAME \n",
    "root_dir = f\"/tmp/finn_dev_{user_name}\"\n",
    "     \n",
    "\n",
    "# -------------------------\n",
    "# Configuración\n",
    "# -------------------------\n",
    "dataset_path = \"Images_test\"  # Carpeta con tus imágenes\n",
    "image_size = (64, 64)  # Cambia según tu dataset\n",
    "num_bits = 8  # Cuantización\n",
    "\n",
    "# -------------------------\n",
    "# Función de cuantización asimétrica\n",
    "# -------------------------\n",
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    min_val = 0\n",
    "    max_val = 2**num_bits - 1\n",
    "\n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / max_val\n",
    "    zero_point = np.clip((-beta / scale), 0, max_val).round().astype(np.int8)\n",
    "\n",
    "    quantized_arr = np.clip(\n",
    "        np.round(arr / scale + zero_point), min_val, max_val\n",
    "    ).astype(np.float32)\n",
    "    return quantized_arr\n",
    "\n",
    "# -------------------------\n",
    "# Cargar una imagen y preprocesar\n",
    "# -------------------------\n",
    "def load_and_quantize_image(file_path):\n",
    "    img = Image.open(file_path).convert(\"RGB\")\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img).astype(np.float32) / 255.0  # normalización a 0-1\n",
    "    img_arr = img_arr.transpose(2, 0, 1)  # (H,W,C) -> (C,H,W)\n",
    "    img_q = asymmetric_quantize(img_arr, num_bits=num_bits)\n",
    "    return img_q\n",
    "\n",
    "# -------------------------\n",
    "# Obtener lista solo de imágenes\n",
    "# -------------------------\n",
    "def get_image_files_recursive(folder):\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(valid_extensions):\n",
    "                image_files.append(os.path.join(root, f))\n",
    "    return image_files\n",
    "\n",
    "image_files = get_image_files_recursive(dataset_path)\n",
    "if not image_files:\n",
    "    raise RuntimeError(\"No se encontraron imágenes en la carpeta.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Crear tensor de entrada usando la primera imagen\n",
    "# -------------------------\n",
    "example_image = load_and_quantize_image(image_files[0])\n",
    "input_t = torch.from_numpy(example_image).unsqueeze(0)  # batch dimension\n",
    "\n",
    "# -------------------------\n",
    "# Exportar modelo Brevitas a QONNX\n",
    "# -------------------------\n",
    "# Asegúrate de que tu modelo cuantizado exista como `net`\n",
    "filename_onnx = os.path.join(root_dir, \"part1.onnx\")\n",
    "filename_clean = os.path.join(root_dir, \"part1_clean.onnx\")\n",
    "\n",
    "export_qonnx(net, export_path=filename_onnx, input_t=input_t)\n",
    "\n",
    "# -------------------------\n",
    "# Limpiar ONNX para FINN\n",
    "# -------------------------\n",
    "qonnx_cleanup(filename_onnx, out_file=filename_clean)\n",
    "\n",
    "# -------------------------\n",
    "# Convertir a FINN\n",
    "# -------------------------\n",
    "model_finn = ModelWrapper(filename_clean)\n",
    "model_finn = model_finn.transform(ConvertQONNXtoFINN())\n",
    "model_finn.save(os.path.join(root_dir, \"ready_finn.onnx\"))\n",
    "\n",
    "print(\"Modelo exportado a FINN en: \" + os.path.join(root_dir, \"ready_finn.onnx\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e23dd2-6ab9-44e7-ba3f-8127cbc48aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
