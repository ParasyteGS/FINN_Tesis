{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53300e8-961f-4385-a466-71eadf0e0018",
   "metadata": {},
   "source": [
    "## Brevitas: Modelo cuantizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0114957-aaba-4f9f-a06b-77c0792c8525",
   "metadata": {},
   "source": [
    "### Modelo 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0925107-9edb-4e72-974e-11a77211e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8WeightPerTensorFloat, Int8ActPerTensorFloat\n",
    "\n",
    "# Variables globales para cambiar la entrada de la imagen\n",
    "W_H_pixels = 64\n",
    "rgb_l = \"L\"\n",
    "\n",
    "\n",
    "# Modelo tipo tinycnn cuantizado, se requirio cambiar CONV2D y MaxPooling a Linear, ya que finn no soportaba ese tipo de capas\n",
    "class QuantTinyCNN_FINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantTinyCNN_FINN, self).__init__()\n",
    "        quantinfo4 = {\"weight_quant\": Int8WeightPerTensorFloat, \"weight_bit_width\": 4} \n",
    "        quantinfo = {\"weight_quant\": Int8WeightPerTensorFloat, \"weight_bit_width\": 8}\n",
    "\n",
    "        self.fc1 = qnn.QuantLinear(\n",
    "            in_features=1 * W_H_pixels * W_H_pixels,  # \n",
    "            out_features=128,\n",
    "            bias=True,\n",
    "            **quantinfo4  # Uso de 4 bits para los pesos en la primera capa, ya que es la más pesada \n",
    "        )\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=4)\n",
    "\n",
    "        self.fc2 = qnn.QuantLinear(\n",
    "            in_features=128,\n",
    "            out_features=64,\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=4)\n",
    "\n",
    "        self.fc3 = qnn.QuantLinear(\n",
    "            in_features=64,\n",
    "            out_features=32,\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=4)\n",
    "\n",
    "        self.fc4 = qnn.QuantLinear(\n",
    "            in_features=32,\n",
    "            out_features=2,  # salida para 2 clases\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplanar entrada (ya no hay conv)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "# \n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a30089c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "# Función de cuantización\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.0\n",
    "    qmax = 2.0**num_bits - 1.0\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    return q_x / qmax  # volver a rango [0,1]\n",
    "\n",
    "\n",
    "class CustomQuantDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_mapping = {\"bird\": 0, \"no_bird\": 1}\n",
    "\n",
    "        # Recorre las carpetas \"bird\" y \"no_bird\"\n",
    "        for class_name, label in class_mapping.items():\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(class_folder, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_files[idx]).convert(\"L\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transform cuantizado\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Aplicar Data Augmentation, rotaciones, flips, cambios de color, normalización\n",
    "transform_quantized = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((W_H_pixels, W_H_pixels)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        # transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3),\n",
    "        transforms.RandomResizedCrop(W_H_pixels, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform_quantized = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((W_H_pixels, W_H_pixels)),\n",
    "        transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "        # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Crear datasets y loaders\n",
    "train_dataset = CustomQuantDataset(\"dataset_split/train\", transform=transform_quantized)\n",
    "val_dataset = CustomQuantDataset(\"dataset_split/val\", transform=val_transform_quantized)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebaf99f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -1.0  /// Max: 1.0\n",
      "dtype: float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Color\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "if rgb_l == \"RGB\":\n",
    "\n",
    "    image, label = train_dataset[12] # tu dataset cuantizado \n",
    "    image = np.array(image) \n",
    "    image = np.transpose(image, (1, 2, 0)) \n",
    "    print(\"Min:\", np.min(image), \" /// Max:\", np.max(image)) \n",
    "    print(\"dtype:\", image.dtype) # Plot \n",
    "    plt.figure() \n",
    "    plt.imshow(image) \n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    \n",
    "    image, label = train_dataset[100]  # imagen monocromática\n",
    "    image = np.array(image)\n",
    "    \n",
    "    # Si tiene forma (1, H, W), elimina el canal\n",
    "    if image.ndim == 3 and image.shape[0] == 1:\n",
    "        image = image[0]  # ahora es (H, W)\n",
    "    \n",
    "    print(\"Min:\", np.min(image), \" /// Max:\", np.max(image))\n",
    "    print(\"dtype:\", image.dtype)\n",
    "    \n",
    "    # Mostrar como imagen en escala de grises\n",
    "    plt.figure()\n",
    "    plt.imshow(image, cmap='gray')  # <- importante para monocromáticas\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76137054-37ed-4e50-9c1a-6bf4dbd6d15e",
   "metadata": {},
   "source": [
    "## Entrenamiento con Dataset Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "252c7412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch [1/70], Loss: 0.6116, Accuracy: 67.88%\n",
      "Epoch [2/70], Loss: 0.5868, Accuracy: 69.12%\n",
      "Epoch [3/70], Loss: 0.5874, Accuracy: 70.25%\n",
      "Epoch [4/70], Loss: 0.5715, Accuracy: 72.50%\n",
      "Epoch [5/70], Loss: 0.5589, Accuracy: 72.00%\n",
      "Epoch [6/70], Loss: 0.5580, Accuracy: 72.38%\n",
      "Epoch [7/70], Loss: 0.5463, Accuracy: 73.62%\n",
      "Epoch [8/70], Loss: 0.5560, Accuracy: 71.62%\n",
      "Epoch [9/70], Loss: 0.5568, Accuracy: 73.25%\n",
      "Epoch [10/70], Loss: 0.5313, Accuracy: 73.75%\n",
      "Epoch [11/70], Loss: 0.5399, Accuracy: 72.25%\n",
      "Epoch [12/70], Loss: 0.5145, Accuracy: 75.50%\n",
      "Epoch [13/70], Loss: 0.5268, Accuracy: 73.25%\n",
      "Epoch [14/70], Loss: 0.5015, Accuracy: 75.38%\n",
      "Epoch [15/70], Loss: 0.5108, Accuracy: 76.62%\n",
      "Epoch [16/70], Loss: 0.4999, Accuracy: 75.12%\n",
      "Epoch [17/70], Loss: 0.4867, Accuracy: 77.50%\n",
      "Epoch [18/70], Loss: 0.4685, Accuracy: 78.75%\n",
      "Epoch [19/70], Loss: 0.4745, Accuracy: 77.38%\n",
      "Epoch [20/70], Loss: 0.4511, Accuracy: 79.00%\n",
      "Epoch [21/70], Loss: 0.4466, Accuracy: 79.00%\n",
      "Epoch [22/70], Loss: 0.4607, Accuracy: 79.38%\n",
      "Epoch [23/70], Loss: 0.4294, Accuracy: 80.12%\n",
      "Epoch [24/70], Loss: 0.4319, Accuracy: 79.00%\n",
      "Epoch [25/70], Loss: 0.4372, Accuracy: 81.75%\n",
      "Epoch [26/70], Loss: 0.4143, Accuracy: 80.00%\n",
      "Epoch [27/70], Loss: 0.4307, Accuracy: 80.00%\n",
      "Epoch [28/70], Loss: 0.4219, Accuracy: 80.88%\n",
      "Epoch [29/70], Loss: 0.4248, Accuracy: 81.50%\n",
      "Epoch [30/70], Loss: 0.3951, Accuracy: 81.88%\n",
      "Epoch [31/70], Loss: 0.3934, Accuracy: 81.12%\n",
      "Epoch [32/70], Loss: 0.3983, Accuracy: 80.75%\n",
      "Epoch [33/70], Loss: 0.3905, Accuracy: 81.75%\n",
      "Epoch [34/70], Loss: 0.3835, Accuracy: 83.62%\n",
      "Epoch [35/70], Loss: 0.3754, Accuracy: 83.25%\n",
      "Epoch [36/70], Loss: 0.3726, Accuracy: 83.38%\n",
      "Epoch [37/70], Loss: 0.3698, Accuracy: 82.50%\n",
      "Epoch [38/70], Loss: 0.3603, Accuracy: 86.12%\n",
      "Epoch [39/70], Loss: 0.3623, Accuracy: 82.88%\n",
      "Epoch [40/70], Loss: 0.3244, Accuracy: 86.50%\n",
      "Epoch [41/70], Loss: 0.3520, Accuracy: 85.38%\n",
      "Epoch [42/70], Loss: 0.3335, Accuracy: 86.75%\n",
      "Epoch [43/70], Loss: 0.3226, Accuracy: 85.50%\n",
      "Epoch [44/70], Loss: 0.3467, Accuracy: 85.12%\n",
      "Epoch [45/70], Loss: 0.2968, Accuracy: 87.62%\n",
      "Epoch [46/70], Loss: 0.3151, Accuracy: 86.75%\n",
      "Epoch [47/70], Loss: 0.3013, Accuracy: 86.50%\n",
      "Epoch [48/70], Loss: 0.3164, Accuracy: 88.25%\n",
      "Epoch [49/70], Loss: 0.3115, Accuracy: 86.88%\n",
      "Epoch [50/70], Loss: 0.2986, Accuracy: 87.75%\n",
      "Epoch [51/70], Loss: 0.3227, Accuracy: 85.88%\n",
      "Epoch [52/70], Loss: 0.2952, Accuracy: 87.75%\n",
      "Epoch [53/70], Loss: 0.2616, Accuracy: 89.88%\n",
      "Epoch [54/70], Loss: 0.2983, Accuracy: 87.50%\n",
      "Epoch [55/70], Loss: 0.3022, Accuracy: 88.00%\n",
      "Epoch [56/70], Loss: 0.2966, Accuracy: 88.62%\n",
      "Epoch [57/70], Loss: 0.2645, Accuracy: 88.38%\n",
      "Epoch [58/70], Loss: 0.2899, Accuracy: 88.25%\n",
      "Epoch [59/70], Loss: 0.2980, Accuracy: 87.38%\n",
      "Epoch [60/70], Loss: 0.3061, Accuracy: 87.12%\n",
      "Epoch [61/70], Loss: 0.2889, Accuracy: 87.38%\n",
      "Epoch [62/70], Loss: 0.2819, Accuracy: 88.75%\n",
      "Epoch [63/70], Loss: 0.2795, Accuracy: 88.38%\n",
      "Epoch [64/70], Loss: 0.2456, Accuracy: 89.38%\n",
      "Epoch [65/70], Loss: 0.2487, Accuracy: 90.25%\n",
      "Epoch [66/70], Loss: 0.2535, Accuracy: 90.00%\n",
      "Epoch [67/70], Loss: 0.2554, Accuracy: 89.25%\n",
      "Epoch [68/70], Loss: 0.2800, Accuracy: 88.38%\n",
      "Epoch [69/70], Loss: 0.2736, Accuracy: 89.12%\n",
      "Epoch [70/70], Loss: 0.2495, Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net = QuantTinyCNN_FINN().to(device)\n",
    "net.train()\n",
    "\n",
    "# Entrenamiento del dataset, aumentar las epocas puede generar un overfitting\n",
    "\n",
    "num_epochs = 70\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_loss = running_loss / len(train_loader)  # pérdida promedio por epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8b89d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Images_test/bird/16.jpg | Label: 0 | Pred: 1\n",
      "Image: Images_test/bird/65a9d7da0d6bb119203b1c13.jpg | Label: 0 | Pred: 1\n",
      "Image: Images_test/bird/9.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/orig-1437426411440.jpg | Label: 0 | Pred: 1\n",
      "Image: Images_test/bird/6.jpeg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/13.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/349664072_639634714750284_7488197136792291295_n.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/7.jpeg | Label: 0 | Pred: 1\n",
      "Image: Images_test/bird/4.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/images_1.jpeg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/pantanos-scaled.jpg | Label: 0 | Pred: 1\n",
      "Image: Images_test/bird/10.jpeg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/8.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/11.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/no_bird/reducIMG_4616.JPG | Label: 1 | Pred: 1\n",
      "Image: Images_test/no_bird/dos.jpg | Label: 1 | Pred: 0\n",
      "Image: Images_test/no_bird/DSC_1491-1024x683.jpg | Label: 1 | Pred: 0\n",
      "Image: Images_test/no_bird/3.jpg | Label: 1 | Pred: 1\n",
      "Image: Images_test/no_bird/paseo-en-catamaran-insonoro.jpg | Label: 1 | Pred: 1\n",
      "Image: Images_test/no_bird/images.jpg | Label: 1 | Pred: 1\n",
      "Accuracy on custom test set: 65.0 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Función de cuantización\n",
    "# -----------------------------\n",
    "def quantize_tensor(tensor, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "    # Evitar división por cero\n",
    "    if min_val == max_val:\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    tensor_q = ((tensor - min_val) / scale).round().clamp(qmin, qmax)\n",
    "    tensor_deq = tensor_q * scale + min_val\n",
    "    return tensor_deq\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Dataset personalizado\n",
    "# -----------------------------\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_mapping = {\"bird\": 0, \"no_bird\": 1}\n",
    "        for class_name, label in class_mapping.items():\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(class_folder, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]   # <- guardar ruta\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, image_path       # <- devolver ruta también\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Transformaciones\n",
    "# -----------------------------\n",
    "\n",
    "# Opción A: sin cuantización\n",
    "transform_test_no_quant = transforms.Compose([\n",
    "    transforms.Resize((W_H_pixels, W_H_pixels)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Opción B: con cuantización\n",
    "transform_test_quant = transforms.Compose([\n",
    "    transforms.Resize((W_H_pixels, W_H_pixels)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "    # transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Dataset y DataLoader\n",
    "# -----------------------------\n",
    "test_dataset = CustomTestDataset(\"Images_test\", transform=transform_test_quant) \n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Evaluación\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = net.to(device)\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, paths in test_loader:   # <- recibe también paths\n",
    "        images, labels = images.to(device).float(), labels.to(device)\n",
    "        out = net(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Image: {paths[0]} | Label: {labels.item()} | Pred: {predicted.item()}\")\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy on custom test set:\", accuracy, \"%\")\n",
    "\n",
    "# Mejora: \n",
    "#  - Aumentar los tipos de imagenes en el dataset para poder tener mejor precision en imagenes IR y con gran cantidad de aves \n",
    "#  - Más imágenes de prueba \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0c02fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantTensor(value=tensor([[-0.0094,  0.0094, -0.0094,  ..., -0.0000,  0.0188,  0.0094],\n",
      "        [ 0.0094,  0.0283,  0.0094,  ...,  0.0188,  0.0188,  0.0188],\n",
      "        [ 0.0094,  0.0094,  0.0000,  ...,  0.0188, -0.0000, -0.0094],\n",
      "        ...,\n",
      "        [ 0.0094,  0.0000,  0.0000,  ...,  0.0283,  0.0094,  0.0188],\n",
      "        [-0.0000, -0.0188, -0.0188,  ..., -0.0000,  0.0094, -0.0094],\n",
      "        [-0.0188, -0.0094, -0.0094,  ...,  0.0000,  0.0094,  0.0188]],\n",
      "       grad_fn=<MulBackward0>), scale=tensor(0.0094, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(4.), signed_t=tensor(True), training_t=tensor(False))\n",
      "tensor([[-1,  1, -1,  ...,  0,  2,  1],\n",
      "        [ 1,  3,  1,  ...,  2,  2,  2],\n",
      "        [ 1,  1,  0,  ...,  2,  0, -1],\n",
      "        ...,\n",
      "        [ 1,  0,  0,  ...,  3,  1,  2],\n",
      "        [ 0, -2, -2,  ...,  0,  1, -1],\n",
      "        [-2, -1, -1,  ...,  0,  1,  2]], dtype=torch.int8)\n",
      "torch.int8\n"
     ]
    }
   ],
   "source": [
    "print(net.fc1.quant_weight())  # pesos cuantizados\n",
    "print(net.fc1.quant_weight().int()) \n",
    "print(net.fc1.quant_weight().int().dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "773a29f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exportado a FINN en: /tmp/finn_dev_parasyte/ready_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parasyte/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "\n",
    "user_name = \"parasyte\"\n",
    "root_dir = f\"/tmp/finn_dev_{user_name}\"\n",
    "     \n",
    "\n",
    "# -------------------------\n",
    "# Configuración\n",
    "# -------------------------\n",
    "dataset_path = \"Images_test\"  \n",
    "image_size = (W_H_pixels, W_H_pixels) \n",
    "num_bits = 8  \n",
    "\n",
    "# -------------------------\n",
    "# Función de cuantización asimétrica\n",
    "# -------------------------\n",
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    min_val = 0\n",
    "    max_val = 2**num_bits - 1\n",
    "\n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / max_val\n",
    "    zero_point = np.clip((-beta / scale), 0, max_val).round().astype(np.int8)\n",
    "\n",
    "    quantized_arr = np.clip(\n",
    "        np.round(arr / scale + zero_point), min_val, max_val\n",
    "    ).astype(np.float32)\n",
    "    return quantized_arr\n",
    "\n",
    "# -------------------------\n",
    "# Cargar una imagen y preprocesar\n",
    "# -------------------------\n",
    "def load_and_quantize_image(file_path):\n",
    "    img = Image.open(file_path).convert(\"L\")\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img).astype(np.float32) / 255.0 \n",
    "    # img_arr = img_arr.transpose(2, 0, 1)\n",
    "    img_arr = np.expand_dims(img_arr, axis=0)  # ahora tiene forma (1, H, W)\n",
    "\n",
    "    img_q = asymmetric_quantize(img_arr, num_bits=num_bits)\n",
    "    return img_q\n",
    "\n",
    "# -------------------------\n",
    "# Obtener lista solo de imágenes\n",
    "# -------------------------\n",
    "def get_image_files_recursive(folder):\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(valid_extensions):\n",
    "                image_files.append(os.path.join(root, f))\n",
    "    return image_files\n",
    "\n",
    "image_files = get_image_files_recursive(dataset_path)\n",
    "if not image_files:\n",
    "    raise RuntimeError(\"No se encontraron imágenes en la carpeta.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Crear tensor de entrada usando la primera imagen\n",
    "# -------------------------\n",
    "example_image = load_and_quantize_image(image_files[0])\n",
    "input_t = torch.from_numpy(example_image).unsqueeze(0)  \n",
    "\n",
    "# -------------------------\n",
    "# Exportar modelo Brevitas a QONNX\n",
    "# -------------------------\n",
    "filename_onnx = os.path.join(root_dir, \"part1.onnx\")\n",
    "filename_clean = os.path.join(root_dir, \"part1_clean.onnx\")\n",
    "\n",
    "export_qonnx(net, export_path=filename_onnx, input_t=input_t)\n",
    "\n",
    "# -------------------------\n",
    "# Limpiar ONNX para FINN\n",
    "# -------------------------\n",
    "qonnx_cleanup(filename_onnx, out_file=filename_clean)\n",
    "\n",
    "# -------------------------\n",
    "# Convertir a FINN\n",
    "# -------------------------\n",
    "model_finn = ModelWrapper(filename_clean)\n",
    "model_finn = model_finn.transform(ConvertQONNXtoFINN())\n",
    "model_finn.save(os.path.join(root_dir, \"ready_finn.onnx\"))\n",
    "\n",
    "print(\"Modelo exportado a FINN en: \" + os.path.join(root_dir, \"ready_finn.onnx\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e23dd2-6ab9-44e7-ba3f-8127cbc48aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
