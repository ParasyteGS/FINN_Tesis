{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9d3362e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch import nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8WeightPerTensorFloat, Int8ActPerTensorFloat\n",
    "\n",
    "\n",
    "W_H_pixels = 32\n",
    "rgb_l = \"RGB\"\n",
    "\n",
    "class QuantTinyCNN_FINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantTinyCNN_FINN, self).__init__()\n",
    "        quantinfo4 = {\"weight_quant\": Int8WeightPerTensorFloat, \"weight_bit_width\": 4}\n",
    "        quantinfo = {\"weight_quant\": Int8WeightPerTensorFloat, \"weight_bit_width\": 8}\n",
    "\n",
    "        self.fc1 = qnn.QuantLinear(\n",
    "            in_features=3 * W_H_pixels * W_H_pixels,  \n",
    "            out_features=128,\n",
    "            bias=True,\n",
    "            **quantinfo4\n",
    "        )\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=4)\n",
    "\n",
    "        self.fc2 = qnn.QuantLinear(\n",
    "            in_features=128,\n",
    "            out_features=64,\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=4)\n",
    "\n",
    "        self.fc3 = qnn.QuantLinear(\n",
    "            in_features=64,\n",
    "            out_features=32,\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=4)\n",
    "\n",
    "        self.fc4 = qnn.QuantLinear(\n",
    "            in_features=32,\n",
    "            out_features=2,  # salida para 2 clases\n",
    "            bias=True,\n",
    "            **quantinfo\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplanar entrada (ya no hay conv)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "# \n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a30089c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "# Función de cuantización\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.0\n",
    "    qmax = 2.0**num_bits - 1.0\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = qmin\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = qmax\n",
    "    else:\n",
    "        zero_point = initial_zero_point\n",
    "\n",
    "    zero_point = int(zero_point)\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(qmin, qmax).round_()\n",
    "    return q_x / qmax  # volver a rango [0,1]\n",
    "\n",
    "\n",
    "class CustomQuantDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_mapping = {\"bird\": 0, \"no_bird\": 1}\n",
    "\n",
    "        # Recorre las carpetas \"bird\" y \"no_bird\"\n",
    "        for class_name, label in class_mapping.items():\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(class_folder, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_files[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transform cuantizado\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_quantized = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((W_H_pixels, W_H_pixels)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3),\n",
    "        transforms.RandomResizedCrop(W_H_pixels, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform_quantized = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((W_H_pixels, W_H_pixels)),\n",
    "        transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Crear datasets y loaders\n",
    "train_dataset = CustomQuantDataset(\"dataset_split/train\", transform=transform_quantized)\n",
    "val_dataset = CustomQuantDataset(\"dataset_split/val\", transform=val_transform_quantized)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ebaf99f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -1.0  /// Max: 1.0\n",
      "dtype: float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Color\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "image, label = train_dataset[12] # tu dataset cuantizado \n",
    "image = np.array(image) \n",
    "image = np.transpose(image, (1, 2, 0)) \n",
    "print(\"Min:\", np.min(image), \" /// Max:\", np.max(image)) \n",
    "print(\"dtype:\", image.dtype) # Plot \n",
    "plt.figure() \n",
    "plt.imshow(image) \n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()\n",
    "\n",
    "## Blanco y negro\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# image, label = train_dataset[10]\n",
    "# image = np.array(image)\n",
    "\n",
    "# # Si es 1 canal (blanco y negro), matplotlib espera shape (H, W)\n",
    "# if image.shape[0] == 1:\n",
    "#     image_to_plot = image[0]  # extraer el canal único\n",
    "# else:\n",
    "#     image_to_plot = np.transpose(image, (1, 2, 0))  # RGB\n",
    "\n",
    "# print(\"Min:\", np.min(image_to_plot), \" /// Max:\", np.max(image_to_plot))\n",
    "# print(\"dtype:\", image_to_plot.dtype)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure()\n",
    "# plt.imshow(image_to_plot, cmap='gray')  # cmap='gray' para blanco y negro\n",
    "# plt.title(f\"Label: {label}\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "252c7412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.6096, Accuracy: 67.62%\n",
      "Epoch [2/50], Loss: 0.5954, Accuracy: 68.25%\n",
      "Epoch [3/50], Loss: 0.5569, Accuracy: 72.75%\n",
      "Epoch [4/50], Loss: 0.5310, Accuracy: 72.88%\n",
      "Epoch [5/50], Loss: 0.5437, Accuracy: 70.38%\n",
      "Epoch [6/50], Loss: 0.5078, Accuracy: 76.12%\n",
      "Epoch [7/50], Loss: 0.5156, Accuracy: 74.00%\n",
      "Epoch [8/50], Loss: 0.5130, Accuracy: 76.38%\n",
      "Epoch [9/50], Loss: 0.5010, Accuracy: 77.00%\n",
      "Epoch [10/50], Loss: 0.5026, Accuracy: 75.12%\n",
      "Epoch [11/50], Loss: 0.4874, Accuracy: 76.25%\n",
      "Epoch [12/50], Loss: 0.4867, Accuracy: 76.25%\n",
      "Epoch [13/50], Loss: 0.4552, Accuracy: 79.25%\n",
      "Epoch [14/50], Loss: 0.4610, Accuracy: 77.62%\n",
      "Epoch [15/50], Loss: 0.4399, Accuracy: 79.50%\n",
      "Epoch [16/50], Loss: 0.4476, Accuracy: 78.62%\n",
      "Epoch [17/50], Loss: 0.4530, Accuracy: 79.75%\n",
      "Epoch [18/50], Loss: 0.4173, Accuracy: 81.00%\n",
      "Epoch [19/50], Loss: 0.4192, Accuracy: 79.75%\n",
      "Epoch [20/50], Loss: 0.4034, Accuracy: 82.12%\n",
      "Epoch [21/50], Loss: 0.4158, Accuracy: 81.00%\n",
      "Epoch [22/50], Loss: 0.4182, Accuracy: 79.00%\n",
      "Epoch [23/50], Loss: 0.3955, Accuracy: 82.38%\n",
      "Epoch [24/50], Loss: 0.4169, Accuracy: 79.38%\n",
      "Epoch [25/50], Loss: 0.3832, Accuracy: 81.75%\n",
      "Epoch [26/50], Loss: 0.3819, Accuracy: 84.38%\n",
      "Epoch [27/50], Loss: 0.3923, Accuracy: 81.38%\n",
      "Epoch [28/50], Loss: 0.3625, Accuracy: 83.62%\n",
      "Epoch [29/50], Loss: 0.3562, Accuracy: 83.75%\n",
      "Epoch [30/50], Loss: 0.3591, Accuracy: 83.75%\n",
      "Epoch [31/50], Loss: 0.3669, Accuracy: 83.62%\n",
      "Epoch [32/50], Loss: 0.3648, Accuracy: 82.50%\n",
      "Epoch [33/50], Loss: 0.3303, Accuracy: 85.38%\n",
      "Epoch [34/50], Loss: 0.3275, Accuracy: 86.25%\n",
      "Epoch [35/50], Loss: 0.3647, Accuracy: 84.12%\n",
      "Epoch [36/50], Loss: 0.3482, Accuracy: 84.62%\n",
      "Epoch [37/50], Loss: 0.3341, Accuracy: 83.75%\n",
      "Epoch [38/50], Loss: 0.3373, Accuracy: 85.12%\n",
      "Epoch [39/50], Loss: 0.3123, Accuracy: 87.38%\n",
      "Epoch [40/50], Loss: 0.3061, Accuracy: 85.88%\n",
      "Epoch [41/50], Loss: 0.2957, Accuracy: 87.12%\n",
      "Epoch [42/50], Loss: 0.2805, Accuracy: 87.75%\n",
      "Epoch [43/50], Loss: 0.2873, Accuracy: 88.75%\n",
      "Epoch [44/50], Loss: 0.3155, Accuracy: 86.00%\n",
      "Epoch [45/50], Loss: 0.3021, Accuracy: 87.88%\n",
      "Epoch [46/50], Loss: 0.2946, Accuracy: 86.38%\n",
      "Epoch [47/50], Loss: 0.2917, Accuracy: 88.25%\n",
      "Epoch [48/50], Loss: 0.3076, Accuracy: 87.38%\n",
      "Epoch [49/50], Loss: 0.2982, Accuracy: 86.50%\n",
      "Epoch [50/50], Loss: 0.3197, Accuracy: 87.50%\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "net = QuantTinyCNN_FINN().to(device)\n",
    "net.train()\n",
    "\n",
    "num_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()  # Para clasificación multiclase\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_loss = running_loss / len(train_loader)  # pérdida promedio por epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8b89d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Images_test/bird/16.jpg | Label: 0 | Pred: 1\n",
      "Image: Images_test/bird/65a9d7da0d6bb119203b1c13.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/9.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/orig-1437426411440.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/6.jpeg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/13.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/349664072_639634714750284_7488197136792291295_n.jpg | Label: 0 | Pred: 1\n",
      "Image: Images_test/bird/7.jpeg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/4.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/images_1.jpeg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/pantanos-scaled.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/10.jpeg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/8.jpg | Label: 0 | Pred: 0\n",
      "Image: Images_test/bird/11.jpg | Label: 0 | Pred: 1\n",
      "Image: Images_test/no_bird/reducIMG_4616.JPG | Label: 1 | Pred: 0\n",
      "Image: Images_test/no_bird/dos.jpg | Label: 1 | Pred: 1\n",
      "Image: Images_test/no_bird/DSC_1491-1024x683.jpg | Label: 1 | Pred: 1\n",
      "Image: Images_test/no_bird/3.jpg | Label: 1 | Pred: 1\n",
      "Image: Images_test/no_bird/paseo-en-catamaran-insonoro.jpg | Label: 1 | Pred: 1\n",
      "Image: Images_test/no_bird/images.jpg | Label: 1 | Pred: 1\n",
      "Accuracy on custom test set: 80.0 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Función de cuantización\n",
    "# -----------------------------\n",
    "def quantize_tensor(tensor, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "    # Evitar división por cero\n",
    "    if min_val == max_val:\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    tensor_q = ((tensor - min_val) / scale).round().clamp(qmin, qmax)\n",
    "    tensor_deq = tensor_q * scale + min_val\n",
    "    return tensor_deq\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Dataset personalizado\n",
    "# -----------------------------\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        class_mapping = {\"bird\": 0, \"no_bird\": 1}\n",
    "        for class_name, label in class_mapping.items():\n",
    "            class_folder = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_folder):\n",
    "                continue\n",
    "            for file in os.listdir(class_folder):\n",
    "                if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    self.image_files.append(os.path.join(class_folder, file))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]   # <- guardar ruta\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, image_path       # <- devolver ruta también\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Transformaciones\n",
    "# -----------------------------\n",
    "\n",
    "# Opción A: sin cuantización\n",
    "transform_test_no_quant = transforms.Compose([\n",
    "    transforms.Resize((W_H_pixels, W_H_pixels)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "# Opción B: con cuantización\n",
    "transform_test_quant = transforms.Compose([\n",
    "    transforms.Resize((W_H_pixels, W_H_pixels)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: quantize_tensor(x, num_bits=8)),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Dataset y DataLoader\n",
    "# -----------------------------\n",
    "test_dataset = CustomTestDataset(\"Images_test\", transform=transform_test_quant)  # prueba primero sin cuantizar\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Evaluación\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Asegúrate de cargar pesos correctos\n",
    "# net = NetClass(...)\n",
    "# net.load_state_dict(torch.load(\"modelo_entrenado.pth\", map_location=device))\n",
    "net = net.to(device)\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, paths in test_loader:   # <- recibe también paths\n",
    "        images, labels = images.to(device).float(), labels.to(device)\n",
    "        out = net(images)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Image: {paths[0]} | Label: {labels.item()} | Pred: {predicted.item()}\")\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"Accuracy on custom test set:\", accuracy, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c02fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantTensor(value=tensor([[ 0.0270,  0.0270,  0.0270,  ...,  0.0000,  0.0000,  0.0270],\n",
      "        [ 0.0540,  0.0270,  0.0000,  ..., -0.0270, -0.0540, -0.0540],\n",
      "        [ 0.0000,  0.0270,  0.0540,  ...,  0.0540,  0.0270,  0.0270],\n",
      "        ...,\n",
      "        [-0.0270, -0.0270, -0.0000,  ...,  0.0270,  0.0540,  0.0000],\n",
      "        [ 0.0270,  0.0000, -0.0270,  ...,  0.0540,  0.0270,  0.0540],\n",
      "        [ 0.0270,  0.0270,  0.0270,  ..., -0.0540, -0.0270, -0.0270]],\n",
      "       grad_fn=<MulBackward0>), scale=tensor(0.0270, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(4.), signed_t=tensor(True), training_t=tensor(False))\n",
      "tensor([[ 1,  1,  1,  ...,  0,  0,  1],\n",
      "        [ 2,  1,  0,  ..., -1, -2, -2],\n",
      "        [ 0,  1,  2,  ...,  2,  1,  1],\n",
      "        ...,\n",
      "        [-1, -1,  0,  ...,  1,  2,  0],\n",
      "        [ 1,  0, -1,  ...,  2,  1,  2],\n",
      "        [ 1,  1,  1,  ..., -2, -1, -1]], dtype=torch.int8)\n",
      "torch.int8\n"
     ]
    }
   ],
   "source": [
    "print(net.fc1.quant_weight())  # pesos cuantizados\n",
    "print(net.fc1.quant_weight().int()) \n",
    "print(net.fc1.quant_weight().int().dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "773a29f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exportado a FINN en: /tmp/finn_dev_parasyte/ready_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parasyte/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "\n",
    "user_name = \"parasyte\" # REPLACE THIS WITH YOUR HOST MACHINE USER NAME \n",
    "root_dir = f\"/tmp/finn_dev_{user_name}\"\n",
    "     \n",
    "\n",
    "# -------------------------\n",
    "# Configuración\n",
    "# -------------------------\n",
    "dataset_path = \"Images_test\"  # Carpeta con tus imágenes\n",
    "image_size = (W_H_pixels, W_H_pixels)  # Cambia según tu dataset\n",
    "num_bits = 8  # Cuantización\n",
    "\n",
    "# -------------------------\n",
    "# Función de cuantización asimétrica\n",
    "# -------------------------\n",
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    min_val = 0\n",
    "    max_val = 2**num_bits - 1\n",
    "\n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / max_val\n",
    "    zero_point = np.clip((-beta / scale), 0, max_val).round().astype(np.int8)\n",
    "\n",
    "    quantized_arr = np.clip(\n",
    "        np.round(arr / scale + zero_point), min_val, max_val\n",
    "    ).astype(np.float32)\n",
    "    return quantized_arr\n",
    "\n",
    "# -------------------------\n",
    "# Cargar una imagen y preprocesar\n",
    "# -------------------------\n",
    "def load_and_quantize_image(file_path):\n",
    "    img = Image.open(file_path).convert(\"RGB\")\n",
    "    img = img.resize(image_size)\n",
    "    img_arr = np.array(img).astype(np.float32) / 255.0  # normalización a 0-1\n",
    "    img_arr = img_arr.transpose(2, 0, 1)  # (H,W,C) -> (C,H,W)\n",
    "    # img_arr = img_arr[np.newaxis, :, :]  # (1,H,W)\n",
    "    img_q = asymmetric_quantize(img_arr, num_bits=num_bits)\n",
    "    return img_q\n",
    "\n",
    "# -------------------------\n",
    "# Obtener lista solo de imágenes\n",
    "# -------------------------\n",
    "def get_image_files_recursive(folder):\n",
    "    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(valid_extensions):\n",
    "                image_files.append(os.path.join(root, f))\n",
    "    return image_files\n",
    "\n",
    "image_files = get_image_files_recursive(dataset_path)\n",
    "if not image_files:\n",
    "    raise RuntimeError(\"No se encontraron imágenes en la carpeta.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Crear tensor de entrada usando la primera imagen\n",
    "# -------------------------\n",
    "example_image = load_and_quantize_image(image_files[0])\n",
    "input_t = torch.from_numpy(example_image).unsqueeze(0)  # batch dimension\n",
    "\n",
    "# -------------------------\n",
    "# Exportar modelo Brevitas a QONNX\n",
    "# -------------------------\n",
    "# Asegúrate de que tu modelo cuantizado exista como `net`\n",
    "filename_onnx = os.path.join(root_dir, \"part1.onnx\")\n",
    "filename_clean = os.path.join(root_dir, \"part1_clean.onnx\")\n",
    "\n",
    "export_qonnx(net, export_path=filename_onnx, input_t=input_t)\n",
    "\n",
    "# -------------------------\n",
    "# Limpiar ONNX para FINN\n",
    "# -------------------------\n",
    "qonnx_cleanup(filename_onnx, out_file=filename_clean)\n",
    "\n",
    "# -------------------------\n",
    "# Convertir a FINN\n",
    "# -------------------------\n",
    "model_finn = ModelWrapper(filename_clean)\n",
    "model_finn = model_finn.transform(ConvertQONNXtoFINN())\n",
    "model_finn.save(os.path.join(root_dir, \"ready_finn.onnx\"))\n",
    "\n",
    "print(\"Modelo exportado a FINN en: \" + os.path.join(root_dir, \"ready_finn.onnx\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e23dd2-6ab9-44e7-ba3f-8127cbc48aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
